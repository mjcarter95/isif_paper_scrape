<html>
<body>
<div class="article-parents">
<ul class="article-journal-name">
<li>Journal of Advances in Information Fusion (JAIF)</li>
<li>Volume 16</li>
<li>Issue 2</li>
<li>Pages 92 - 113</li>
<li>December, 2021</li>
<li class="article-doi"><a href="https://doi.org/">https://doi.org/</a></li>
</ul>
</div>

<div class="article-header">
<h1 class="article-title">ML-PMH Tracking in Three Dimensions Using Cluttered Measurements From Multiple Two-Dimensional Sensors</h1>
<p class="article-authors">ZACHARIAH  SUTTON, PETER  WILLETT, TIM  FAIR and YAAKOV  BAR-SHALOM</p>
</div>

<div class="article-url">
<a href="https://confcats_isif.s3.amazonaws.com/web-files/journals/entries/052021-0009R1.pdf">Read Article</a>
</div>
<div id="articleBody" class="article-section">
<h2 class="article-heading" id="Abstract">Abstract</h2>
<p> ment of Electrical and Computer Engineering, University of Con-  necticut, Storrs, CT, USA E-mail: zachariah.sutton@uconn.edu,  peter.willett@uconn.edu, yaakov.bar-shalom@uconn.edu  T. Fair is from Toyon Research Corporation, E-mail: tfair@toyon.com    This work was partially supported byAFOSR under contract FA9500-  18-1-0463.    1557-6418/21/$17.00 © 2021 JAIF    ML-PMH Tracking in Three  Dimensions Using Cluttered  Measurements From Multiple  Two-Dimensional Sensors    ZACHARIAH SUTTON  PETERWILLETT  TIM FAIR  YAAKOV BAR-SHALOM    The maximum-likelihood probabilistic multi-hypothesis tracker    (ML-PMHT) is a tracking method whose flexibility and scalability de-    rive from relinquishing the assumption that each target emits at most    one “hit” per scan of the sensor. This is an ML method that essen-    tially reduces to an optimization problem—recursively maximizing a    likelihood function that is simple to evaluate given a batch of observa-    tions. Unlike maximum a posteriori or minimum mean squared error    (MMSE) trackers, this likelihood maximization tracker requires nei-    ther prior knowledge about target motion nor measurement associ-    ation, making it conceptually easy to work with. Here, this method is    used to track targets in a three-dimensional “global” space with obser-    vations provided bymultiple two-dimensional sensors placed through-    out the global space. Since the observation model is non-linear, the    likelihood maximization is done via hill climbing. For this purpose, we    also address the issue of “hill finding.” Due to the presence of clutter    in the measurement model, the likelihood is a multi-modal function of    the parameter space. That is, there are multiple hills in the likelihood    function, and it is of great advantage to the tracker to initialize the hill    climber close to the right hill—the one whose peak is the global maxi-    mum. In this work, we present a data-driven method of initializing the    hill climber based on the received observations.    I. BACKGROUND    The maximum-likelihood probabilistic multihy-  pothesis tracker (ML-PMHT) is an ML target track-  ing paradigm that is convenient in cases where data  association—the measurement-to-target assignment  processes prior to updating the estimate—involves  significant numerical complexity, generally (but not  always) due to heavy clutter. In some settings, it is  possible that a single target will result in multiple mea-  surements at a particular sensor and time (tracking of  “extended objects,” for instance). In such cases, filters  that employ “hard” data association (the JPDA [20]  and random finite set filters such as the multi-Bernoulli  [6], [15]) will be sub-optimal since they make the fun-  damental assumption that each target being tracked  produces at most one measurement per sensor per time  step. In contrast, the likelihood function used in the  ML-PMHT is formulated by considering each mea-  surement individually, and applying a probability mass  function over the possible measurement generating pro-  cesses (targets and clutter). That is, instead of assuming  that a particular measurement has come from a partic-  ular target and evaluating the measurement likelihood  with that assignment, the ML-PMHT formulates the  measurement likelihood with a “soft” assignment that  accounts for uncertainty as to the process from which  a particular measurement originated. This formulation  naturally allows for the possibility that a target has  originated multiple measurements in a single “scan” of  a sensor. Along with being a better representation of  reality in some settings, the soft assignment also avoids  the computational bookkeeping cost of the hard as-  signment problem that the data association filters must  solve for each scan with relatively expensive routines  like Murty’s k-Best Assignment Algorithm [12]. Thus,  the ML-PMHT approach may also be desirable in some  settings where computational cost is a consideration.    The ML-PMHT likelihood formulation is borrowed  from the PMHT framework [7], [10]. The ML-PMHT  differs, however, in that it treats the target state (joint  target state in the case of multiple targets) as an un-  known deterministic parameter, and obtains anML esti-  mate of the parameter based on batches ofmeasurement  scans. It has shown especially good performance in sce-  narios with high levels of clutter [14], [21].    This work will use the ML-PMHT to perform data-  batch-based tracking of targets in a three-dimensional  “global” space based on multiple passive sensors that  return two-dimensional measurements. A generalized  measurement model is presented that can be adapted to  any type of sensor that returns measurements that can  be transformed into lines-of-sight.Some common sensor  types that could be used with this model are focal plane  arrays (cameras) with measurements given in the two-  dimensional image space, or passive radars that return  azimuth and elevation angles (or azimuth and elevation  angle sines).    92 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        It is assumed that all sensors report to a central pro-  cessor that performs likelihood maximization based on  all observed data. The central optimization reveals an-  other benefit of the ML-PMHT in that data from multi-  ple sensors are naturally included in the likelihood for-  mulation in a simple linear sum manner. While sequen-  tial updating over sensors is a common practice in data  association filters, it is theoretically sub-optimal [20]; and  optimality withmultiple sensors according to the rules of  “hard” data association is computationally costly.    The ML-PMHT reduces to a conceptually straight-  forward optimization problem where likelihood maxi-  mization happens over a multi-dimensional parameter  space. However, an important practical consideration  in optimization problems is initialization: how to pick  the “initial guess” for the parameter value. Initializa-  tion is particularly important in the tracking setting since  we assume the presence of clutter measurements, which  makes the likelihood multi-modal. That is, coinciden-  tal “patterns” in clutter measurements can lead to false  maximums in the likelihood value that gradient-based  maximizers will reach in error if not initialized carefully.  Although the global maximum tends to occur at the true  parameter value, a maximizer must be initialized suffi-  ciently close to the global maximum in order to reach  it. For this purpose, a “hill finding” method is presented  where received measurements are used to identify sta-  tistically significant points in the global parameter space  that can be used to initialize the maximization. This  hill finding routine is conceptually separate from the  ML-PMHT, and is perhaps the most novel contribution  of this work. The method represents a means of identi-  fying pairs of line-of-sight measurements from separate  sensors that strongly correlate to a single point in three-  dimensional space.Thus, it could theoretically be used in  other settings where one would wish to identify points in  three-dimensional space that are statistically supported  by lower-dimensional measurements. (It could inform  the “target birth”process in themulti-Bernoulli filter, for  example.)    The benefit of central data processing and the re-  sulting ability to perform the “hill finding” is demon-  strated by comparing the method to a decentralized op-  tion where ML estimates are obtained individually by  each sensor, then fused in the global space. It is shown  that centralized optimization has a significant advantage  in settings with low target visibility. A comparison is  alsomade to the joint probabilistic data association filter  (JPDAF).    The paper is structured as follows. The models  used in the work are presented in Section II, in-  cluding the target parameter model, the measurement  model, and the model of the geometric arrangement  of sensors. The ML-PMHT likelihood formula is given  in Section III. The “hill finder” is presented in Sec-  tion IV, and a step-by-step summary of the overall  ML-PMHT method is given. Simulated results are  shown in Section V.    II. MODELING ASSUMPTIONS    Themodel assumes a three-dimensional global space  in which targets are to be tracked.Measurements are re-  ceived from a group ofNs sensors distributed around the  global space. This work uses a conventional Cartesian  coordinate system in the global space, but in theory it  could be replaced by a local north-east-down reference  frame or any other space where the following conditions  are fulfilled:    1) Themotion of targets can be (approximately) param-  eterized in the space.    2) The pose of every sensor is known in the space.    The pose of a sensor parameterizes the transfor-  mation between the global coordinate system and the  sensor’s coordinate system. The sensors are assumed  to have six degrees of freedom (DOFs)—three transla-  tional (location) and three rotational (pointing). So the  pose consists of six known parameters for each sensor.  If a sensor’s pose changes over time, it is assumed to be  known for each point in time that a measurement is re-  ceived. The conventions used for the pose and the re-  sulting transformations are discussed in more detail in  Section II.C.    A. Target Motion    The user must choose a batch size parameter Nb,  which is the number of scans from each sensor that will  be used in the likelihood evaluation. It is assumed that,  for all targets, the true target motion can be reasonably  approximated by a constant velocity model over the du-  ration of the batch. That is, for any discrete global time  index k, the motion of target j over the pastNb − 1 sam-  pling periods (Nb sampling points) is given by    X j,� � (X j,k − X j,k−Nb+1)  n    Nb − 1 + X j,k−Nb+1    n = 0, . . . ,Nb − 1,    (1)    where X j,� denotes a three-element column vector con-  taining the target’s position in the global Cartesian space  at global time step � and n is a local time index such that  � = k−Nb + 1 + n.    A constant sampling period is assumed here, but  there is no loss of generality.With the batch size chosen,  the motion in (1) is entirely parameterized by X j,k−Nb+1  and X j,k—the positions of target j at the start and end  times of the batch. Thus, the motion to be estimated via  likelihood maximization can be described with six pa-  rameters for each target. For a scenario with Nt targets    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 93        present, form a parameter vector    X k �    ⎡  ⎢⎢⎢⎢⎢⎢⎢⎣    X1,k−Nb+1    X1,k  ...    XNt,k−Nb+1    XNt,k    ⎤  ⎥⎥⎥⎥⎥⎥⎥⎦    . (2)    We ultimately will be maximizing the log likelihood  function over this vector given Zk—the batch of mea-  surements up to and including time step k—to obtain the  estimate    X kML = arg max  X k∈IR6Nt    L(X k;Zk). (3)    The definition of Zk and the formulation of the likeli-  hood function L(·) are discussed later. The maximiza-  tion is mentioned here to emphasize that the dimension-  ality of the space over which maximization is performed  increases by 6 for each additional target. This is a com-  putational consideration in a practical application.    In theory, this likelihood maximization requires only  the constant velocity assumption over the course of any  particular batch.However, it will be of use if the user has  somemore prior knowledge about targetmotion.Specif-  ically, if there is knowledge available about the range of  possible target speeds, it will prove useful in the initial-  ization of the hill climber as discussed in Section IV.A.    Note that the particular six-parameter motion model  used here is not the only model compatible with batch  tracking. One could also choose—at the cost of compu-  tation time—to use a nine-parameter (initial position,  initial velocity, and acceleration) model or any other  method of parameterizing the targetmotion over the du-  ration of the batch.    B. Measurement Model    The tracker developed in this work uses line-of-sight  measurements. That is, it is assumed that each and ev-  ery sensor returns some form of two-element measure-  ments that can be used to parameterize a line-of-sight  beginning at the origin of its own coordinate system and  extending infinitely in the direction of sight. The line-  of-sight measurement model is a fundamental feature  of this work. However, there are multiple types of two-  dimensional measurements that provide a line-of-sight,  which allows the sensor type to remain ambiguous. Sim-  ple passive radar models directly provide line-of-sight  measurements.Also, a point in the image space of a cam-  era can be converted into a line-of-sight given the cam-  era model. The simplicity of radars in this context ren-  ders them rather uninteresting. Thus, cameras are as-  sumed in the peripheral theoretical modeling and verifi-  cation in this work, without loss of generalization in the  fundamental aspects of the work (likelihood maximiza-  tion).This section discusses the relationship between im-    Fig. 1. Illustration of the sensor reference frame convention used in  this work. Two different measurement types are shown. Given the  coordinates of the vector x, one could solve for θ̄ and φ̄, which is the  azimuth–elevation measurement model assumed throughout this    work. Given a point in the image plane (shaded) at a known distance  f from the origin (where f has the same length units as η, ζ    coordinate system), one could also calculate the corresponding  azimuth and elevation angles.    age space and line-of-sight measurements within the co-  ordinate system of a sensor’s reference frame.The trans-  formation between a sensor’s coordinate system and the  global coordinate system is discussed in Section II.C.    When camera images are used with this algorithm,  it is necessary to first perform measurement extrac-  tion. The assumption when using cameras as sensors is  that targets have a contrasting appearance to the back-  ground. The extraction must find significantly bright or  dark spots in an image, and condense each spot down  to a point in the image space of the camera, which, in  turn, can be converted into line-of-sight measurements  via the camera model.Methods of extracting these mea-  surements from images are discussed in [3] and [11]. The  signal-to-noise ratio (SNR) of the scenario is an impor-  tant factor in themeasurement extraction step.Note that  a significant portion of [3] is dedicated to defining the  SNR. The extraction process will have a certain proba-  bility of extracting “false” measurements, which we will  refer to as clutter.    Once measurement extraction has been performed  on the images, the resulting measurements must be con-  verted from the image space into lines-of-sight. Refer  to Fig. 1 for a summary of this conversion. The camera  model used here is the pinhole projectionmodel [5], [23].  The image is treated as a plane parallel to the x–y plane  of the sensor coordinate frame, set at some non-zero fo-  cal distance f along the sensor z axis. Let the coordinate  system in the image plane be denoted by (η, ζ ), which  is centered on the sensor z axis, and has directional con-  vention that agrees with the typical row–column format  of images. Then a point [η, ζ ]′ in the image plane coor-  dinate system has location [η,−ζ , f ]′ in the coordinate    94 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        system of the sensor’s reference frame.Note that the im-  age plane is treated as a continuous space (as opposed  to a quantized space) since the measurement extraction  process yields continuous values.Azimuth–elevation an-  gles are used for parameterizing line-of-sight measure-  ments. The azimuth angle θ is taken in the sensor y–z  plane, with zero defined as the positive z axis and the  positive direction defined as from the positive z axis to  the positive y axis. The elevation angle φ is the angle be-  tween the sensor y–z plane and the positive x axis, de-  fined as zero on the y–z plane and positive toward the  positive x axis. Under this choice of convention, a given  point in the image plane [η, ζ ]′ yields line-of-sight angles    θ = tan−1  (−ζ    f    )  , (4)    φ = tan−1  (    η√  ζ 2 + f 2    )  . (5)    Note that this particular cameramodel is somewhat sim-  plistic. It does not account for image distortion or other  practical effects. If the user has a more accurate model  of the cameras, it will be compatible with this algorithm  so long as it provides a way to obtain lines-of-sight from  points in the image.Since the algorithmultimately works  with line-of-sight measurements, this writing will occa-  sionally use the general term “measurements” when re-  ferring to azimuth–elevation measurements.    Overall, it is assumed that at a particular time, each  sensor i returns a set of line-of-sight measurements—  potentially after conversion with (4) and (5)—which in-  cludes any target-originated measurements along with  any measurements originating from the clutter process.  That is, the set of measurements returned by sensor i at  time � can be denoted    Zi,� = {zi,�,m}N  z  i,�    m=1 =  {[    θi,�,m  φi,�,m    ]}Nzi,�  m=1    , (6)    where each zi,�,m is a two-element column vector and  Nzi,� is the number of measurements at the current  time/sensor.    It will be helpful later on, during formulation of the  likelihood function, to have a simple expected value  parameterization for the number of clutter and target-  originating measurements. The expected numbers of re-  ceivedmeasurements are fundamentally tied to themea-  surement extraction process, which is left non-specific  for most of this work. Let us assume some general ex-  traction process such that λi,� and ϕi,� are the expected  numbers of clutter measurements and target-originated  measurements, respectively, in the scan from sensor i at  time �. For the sake of more generality, allow each indi-  vidual target j to originate a potentially unique expected  number of measurements ϕi, j,� such that    ϕi,� =  ∑  j    ϕi, j,�. (7)    For purposes of simulation later in this work, it will  be assumed that the number of clutter measurements is  Poisson with some expected value λi. It will be further  assumed that, independently for each target present,  sensor i either reports a single measurement with “de-  tection probability” pd,i or “misses” the target. Thus, the  expected number of target-originated measurements  from each target is pd,i, and ϕi,� = pd,iNt for all �. These  assumptions are made to fit with a typical model used  in other trackers for the sake of comparison. However,  one of the main benefits of the ML-PMHT likelihood  formulation is that it is more flexible than trackers  that consider one-to-one data associations. Whereas  the JPDA and its derivative algorithms must make  the fundamental assumption that each target produces  at most one measurement per scan, the ML-PMHT  formulation requires no such assumption. Some data  association tracking methods do exist for extended  targets (targets that produce more than one measure-  ment in a single scan), and usually involve recursive  estimation of properties (e.g., shape, size) of targets. The  ML-PMHT offers a relatively cheap way around this  extra estimation for the case when the extended target  measurement assumption is true, but the shape/size  properties of targets are not of particular interest. For  instance, a simple model could assume that the number  of measurements originating from target j is Poisson  random number with expected value ϕi, j.    It is assumed that any target-originated measure-  ments have a random additive measurement error. Since  the ML-PMHT is an objective function optimization  problem, the parameterization of the measurement er-  ror is somewhat flexible:Any objectively computable er-  ror probability density can be used. This work will use  the typical Gaussian error assumption. That is, if a par-  ticular measurement zi,�,m originates from target j, then  it is a random vector given by    zi,�,m = z̄i, j,� + νi, j,�, (8)    where z̄i, j,� is the noiseless (zero-error) measurement by  sensor i due to target j at time �.Given the three-element  vector xi, j,� representing the Cartesian (x–y–z) position  of target j in the sensor i coordinate frame at time �, the  noiselessmeasurement for the azimuth–elevationmodel  is given by    z̄i, j,� =  [    θ̄i, j,�  φ̄i, j,�    ]  =    ⎡  ⎢⎣ tan    −1  (  yi, j,�  zi, j,�    )  tan−1    (  xi, j,�√    y2i, j,�+z2i, j,�    )  ⎤  ⎥⎦, (9)    where xi, j,�, yi, j,�, zi, j,� are the individual components  of xi, j,�. For a visual representation of this measurement  model refer to Fig. 1 and treat the orange vector as the  target position. The term νi, j,� in (8) is a two-element  multivariate random Gaussian vector with distribution    νi, j,� ∼ N (0,Ri,�), (10)    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 95        (a () b () c)    Fig. 2. Step-by-step illustration of a three-DOF proper Euler rotation. The total rotation is the result of applying three elemental rotations  successively. The specific convention shown here and assumed in this work is a z–y–z intrinsic rotation. The focal plane shown in the figures    matches the orientation of the one shown in Fig. 1. The origin of the (u, v) coordinate system is marked by a dot. The rotations bring the plane  from the orientation marked by the red dot in (a) to the orientation marked by the black dot in (c). Orientations that are marked with a    common color are the same. Colored circles attempt to show planes of rotation.    where, in general, the covariance matrix Ri,� is allowed  to change from one sensor to the next and one time step  to the next. Measurement errors are assumed to be in-  dependent between sensors and between time steps.    Clutter measurements are assumed to be uniformly  distributed in the measurement space. It has been as-  sumed in this work that each sensor has a limited field  of view, modeled by setting limits on the azimuth and  elevation angles symmetrically around zero. Then for a  sensor i with a total azimuth field of viewW θi and total  elevation field of viewWφi , the spatial distributions of the  individual components of clutter measurements are    θ  clutter  i ∼ U (−W θi /2, W θi /2), (11)    φ  clutter  i ∼ U    (  −Wφi /2, W    φ    i /2  )    , (12)    where, in general, different sensors are allowed to have  differently sized fields of view—hence the indexing with  i. For sensors with reasonably narrow fields of view  (within the range of realistic cameras), the uniform dis-  tributions in azimuth and elevation result in image space  measurements that are very close to uniform in the im-  age plane. See Figs. 7 and 8 for a visual example of mea-  surements in an image plane resulting from this model.    The total measurement batch Zk in (3) can be ex-  pressed as    Zk = {{Zi,�}i=N  s    i=1 }�=k�=k−Nb+1. (13)  Or, in words,Zk is the set of all subsets of measurements  (both target and clutter originated) from all sensors for  the most recent Nb sample times (up to and including  the current estimation time k).    C. Sensor-World Setup    Notice that in Fig.1 and in the formulas in (8) and (9),  it is assumed that xi, j,�—the three-dimensional Cartesian  position of target j in the sensor i reference frame at    time �—is given. Since the likelihood in (3) is being max-  imized over targetmotion parameters given in the global  reference frame, the relationship between the global ref-  erence frame and each sensor reference frame must be  defined.    Let the axes of the global coordinate system be de-  noted (X−Y−Z) and those of the sensor coordinate sys-  tem (x−y−z). The pose of the sensor is the position and  orientation of its reference frame relative to the global  reference frame, defined such that at a pose of zero, the  two coordinate systems are one and the same.    Let the three rotational DOFs be described by the  angles αi,�, βi,�, and γi,�, where the time index � is noted  since the rotation of a sensor can changewith time.There  are multiple choices of convention for the actual mean-  ing of these angles.To be exact, there are 12 unique ways  to describe every possible orientation in terms of three  angles.    The rotation convention used here is illustrated in  Fig. 2, where, for convenience, a particular sensor at a  particular time is considered and the indexing ismomen-  tarily dropped. The sensor coordinate system is initially  aligned with the global coordinate system. The overall  rotation is the combination of three intrinsic rotations  performed sequentially.    (a) A rotation by α around the Z axis results in the new  coordinate system (x′–y′–z′).    (b) Then a rotation by β around the y′ axis to obtain  (x′′–y′′–z′′).    (c) Finally, a rotation by γ around the z′′ axis gives the  fully rotated coordinate system (x′′′–y′′′–z′′′).    Here, the positive direction for all rotations is given  by the “right-hand rule.” This describes what is com-  monly called an z–y–z intrinsic rotation. Here z–y–z  refers to the sequence of rotation axes, and intrinsic  refers to the fact that successive rotations are performed  around the axes of the rotating coordinate system (sen-  sor coordinate system) itself as opposed to rotating    96 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        Fig. 3. Illustration of sensor-world setup for a particular sensor,  target, and time step. Given target location X in the global (X–Y–Z)    coordinates and sensor pose information �, ξ, the zero-error  measurements θ̄ , φ̄ in (9) are calculated by first evaluating x—the    target location in sensor (x–y–z) coordinates.    around the axes of the fixed global coordinate system.  There is nothing particularly special about the choice of  the z–y–z convention; in light of the camera model in  Fig. 1, it is merely a way that one could conceivably go  about orienting such a camera in a practical scenario. If  the rotation angles are in some other convention, they  will work just as well, and only the rotation matrix will  change. For a comprehensive description of Euler rota-  tions, see [22].    Referring now to Fig. 3, let the three-dimensional  Cartesian position of sensor i at time step � be a vector  denoted by ξi,�.After rotating the sensor coordinate sys-  tem, the final sensor coordinate system is given by trans-  lating the origin of the fully rotated system into the point  ξi,�.    Now,givenX j,�—aposition vector for target j at time  � in the global reference frame—the resulting target po-  sition in the reference frame of sensor i is given by the  inversing transformation    xi, j,� = �−1i,� (X j,� − ξi,�). (14)  Here, �i,� is the 3 × 3 rotation matrix that transforms  Cartesian points from the fixed global frame to the ro-  tated (but non-translated) reference frame. For the par-  ticular rotation convention used in this work, the rota-  tion matrix is given by    �i,� =  ⎡  ⎣ cosα cosβ cos γ − sinα sin γ − cos γ sinα − cosα cosβ sin γ cosα sinβcosα sin γ + cosβ cos γ sinα cosα cos γ − cosβ sinα sin γ sinα sinβ    − cos γ sinβ sinβ sin γ cosβ    ⎤  ⎦ , (15)    where the (i, �) indexing is removed from the angles for  convenience. Note that �i,� entirely describes the rota-  tion of sensor i at time �. That is, the poses of all sensors  can be recorded as the set of data matrices    S = {�i,�, ξi,�}N  s    i=1, ∀�. (16)    III. LIKELIHOOD EVALUATION    A fundamental feature of the ML-PMHT is that no  hard limit is assumed for the maximum number of mea-  surements originating from any one target in any one  sensor at any one sample time. This is a significant de-  parture from the assumptions made in data association  filters where various one-to-one measurement-to-target  assignment events are enumerated and considered. This  modeling relaxation allows the ML-PMHT formulation  to consider any particular measurement (indexedm) in-  dependently from all other measurements, and assign  a prior probability mass function over the set of possi-  ble measurement generating processes (clutter and all  targets)    �i,� = (πi, j,�)N  t    j=0, s.t.  Nt∑  j=0    πi, j,� = 1, (17)    where πi, j,� is the prior probability that any particular  measurement in the scan from sensor i originated from  process j, and j = 0 indicates the clutter process. For  now,allow the value of the priors to be ambiguous.Meth-  ods for setting the priors are discussed in Section III.A.    The most conveniently scaled statistic to maximize  is the log-likelihood ratio (LLR) of the target state X k  based on the measurement batch Zk. By definition, the  LLR is given by    L(X k;Zk) = ln  (  p(Zk|X k)  p(Zk|∅)    )  , (18)    where p(Zk|∅) represents the probability density func-  tion (pdf) of the measurement batch given that no tar-  gets are present—the pdf of the entire batch given that  everything is clutter-generated.Under the measurement  independence assumptions and the product to sum log-  arithm property, (18) can be written as    L(X k;Zk) =  Ns∑  i=1    k∑  �=k−Tb    Nzi,�∑  m=1    ln    (  p(zi,�,m|X k)  p(zi,�,m|∅)    )  , (19)    where Nzi,� is the number of measurements in the scan  of sensor i at time step �. The term p(zi,�,m|∅) is the pdf  of a singlemeasurement given that it originated from the  clutter process.Under the simplifying assumption in (11)    and (12), this pdf is uniform in azimuth–elevation space,  given by    p(zi,�,m|∅) =  1  Vi    = 1  W θi W    φ    i    , ∀ �,m, (20)    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 97        where Vi is the total volume of the measurement space  of sensor i.    Under the ML-PMHT framework, the term in the  numerator on the right-hand side of (19) is given by    p(zi,�,m|X k) = πi,0,� p(zi,�,m|∅) +  Nt∑  j=1    πi, j,� p(zi,�,m|X j,�),    (21)  which is a convex combination of the likelihoods based  on the different possible measurement generating pro-  cesses with the coefficients being the prior probabilities  in (17). The term X j,� is the global position of target j at  time step � and is given in (1). The relation betweenX j,�  and the batch joint target motion vector X k is given by  (1) and (2).    Under the Gaussian measurement error assumption,  the term p(zi,�,m|X j,�) is given by the multivariate Gaus-  sian density    p(zi,�,m|X j,�) =  1√    |2πRi,�|  e−0.5(zi,�,m−z̄i, j,�)    TR−1i,� (zi,�,m−z̄i, j,�).    (22)  Under the azimuth–elevation measurement model,  z̄i, j,�—the predicted measurement originating from tar-  get j at time step � from sensor i—is given by (9). The  sensor reference frame target position xi, j,� required in  the measurement prediction is given in terms of the  global target position X j,� by (14).    Combining (19)–(22) gives a final expression for the  LLR in (23).    L(X k;Zk) =  Ns∑  i=1    k∑  �=k−Tb    Nzi,�∑  m=1    ln    ⎛  ⎝πi,0,� +ViCi,�    Nt∑  j=1    πi, j,� e  −0.5(zi,�,m−z̄i, j,�)TR−1i,� (zi,�,m−z̄i, j,�)    ⎞  ⎠ . (23)    In (23),Ci,� is the Gaussian constant given by    Ci,� =  1√    |2πRi,�|  . (24)    Since the remainder of this work deals with maximizing  the LLR over the target motion space given a batch of  measurements at time step k, the function in (23) will be  denoted as L(X k) for simplicity.    A. Choice of Priors    The performance of the ML-PMHT has been shown  in pioneering works to be rather robust to changes in the  prior probabilities in (17), (21), and (23). Qualitatively  speaking, if it is expected that most of the measurements  in any one scan will be clutter, the prior for the clutter  process should be significantly higher than the priors for  targets.    The simplest option is to naively set the clutter prior  based on expected values. If λi,� is the expected num-  ber of clutter measurements from sensor i at time � and  ϕi, j,� is the expected number of measurements originat-  ing from target j, the prior for the clutter process can    reasonably be set as    πi,0,� =  λi,�    λi,� +  ∑    j ϕi, j,�  , (25)    and the prior for target j as    πi, j,� =  ϕi, j,�    λi,� +  ∑    j ϕi, j,�  . (26)    The priors also provide a convenient way of working  with sensors with restricted fields of view. If, for instance,  a target position xi, j,� in the reference frame of sensor i is  such that either of the corresponding line-of-sight angles  is out of the sensor’s angular range (introduced in Sec-  tion II.A to model a restricted field of view), then the  corresponding “predicted observation” is z̄i, j,k = ∅. That  is, target j is expected to be out of view of sensor i at  time �.A convenient way to deal with this is to simply set  πi, j,� = 0,and adjust the other priors so that    ∑  j πi, j,� = 1.    IV. LIKELIHOOD MAXIMIZATION    As stated earlier in (3), the ML batch estimate up to  time k is the X k that maximizes L(X k). Since L(X k)  is highly non-linear, an analytic solution is not obtain-  able.However, given a batch of measurements and some  fixed X k0, it is easy enough to evaluate L(X k0 ) using (9),  (14),and (23).Therefore,maximization can be done via a  hill climbing algorithm combined with other techniques  (discussed later) to get within the neighborhood of the  global maximum.    Generally speaking,any hill climbing algorithm func-  tions by stepping around a parameter space attempting  to find the global maximum in some function of the pa-  rameters. Obviously, the hill climber must be started at  some initial point in the parameter space. For some ap-  plications, it would be perfectly reasonable to sample the  initial point from a uniform distribution on the parame-  ter space. In other applications, the measurement space  and the parameter space are one and the same. In such  cases,onemay simply treat some observedmeasurement  as the initial step in the hill climber. There are two main  challenges in the likelihood maximization in this work.  First, the measurement spaces are not the same as the  parameter space.Second,due to clutter, there is not a sin-  gle hill in the LLR (see Fig.4). Instead, there aremultiple  “false” hills (local maxima) along with a single true hill  (global maximum). In general, the true hill will be taller  than the false hills. However, from the point of view of  the hill climber, there is no way to determine how tall  a given hill is at the start of climbing. This means that  the climber can get stuck climbing the wrong hill. Thus,  it would be to our advantage to have a method of ini-  tializing the climber as close to the true hill peak as pos-    98 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        Fig. 4. The LLR surface centered on truth with two out of six  dimensions varied (the global X coordinate of both the start and end  point of the one target). The true hill (global max) is significantly    taller than the rest (local maxima). But since hill climber termination  is based on hill slope, the climber could potentially terminate at the  top of a false hill. This demonstrates why it is important to initialize    the climber as close to the true hill as possible.    sible. This routine will be referred to as “hill finding” in  this work. However, “hill finding” is somewhat of a mis-  nomer since the LLR surface has many hills. Precisely  speaking, the routine is an attempt to get close (in the  parameter space) to the one hill whose peak is the global  maximum; the term “hill finding” is used for concision.    A. Hill Finding    The parameter space in this application is described  by (2). For a scenario with Nt targets, the parameter ex-  ists in a 6Nt dimensional space. Each six-dimensional  sub-space parameterizes a line segment in the three-  dimensional global tracking space—three dimensions  for the start point and three for the end point. The line  segments parameterized in this space will represent the  estimate of the corresponding target’s trajectory over  the course of a batch.With this formulation, a “point” in  the parameter space represents a group of Nt line seg-  ments in the tracking space. Maximizing the likelihood  over this parameter space amounts to finding the group  of line segments that best represents the target trajecto-  ries based the batch of measurements.    “Good” initialization of the hill climber involves set-  ting the initial parameter vector to represent a “good  guess” as to the target trajectory segment(s) during the  batch. Given some prior knowledge of the targets’ be-  havior, it is possible to predict a parameter to use for ini-  tializing the climber.However, for the sake of robustness  and to deal with scenarios where no prior information  about the targets is available, we have developed a data-  drivenmethod of initializing the hill finder.This method,  detailed in the rest of this section, obtains a parame-    ter initialization based only on the currently observed  batch of measurements. Thus, the tracker can be started  “blind”—with no prior information about the targets—  and the data-driven hill finding should cause the tracker  to converge on the target track(s) within a few batches.  Based on the formulation of the parameter space, the ul-  timate goal of the hill finder—presented in the following,  somewhat verbose discussion—is to obtain estimates of  target locations in the three-dimensional tracking space  at the start and end times of the batch of measurements.    Given the pose of sensor i at time step �, {�i,�, ξi,�},  and a single two-dimensional measurement zi,�,m from  that sensor and time, there is not enough information  present to solve for a potential target location in three-  dimensional global space. There is, however, enough in-  formation to define a ray in three-dimensional global  space that starts at the origin of the sensor coordi-  nate system and extends infinitely in the line-of-sight  direction indicated by the measurement. The azimuth–  elevationmeasurement model is particularly convenient  here since it directly gives the azimuth and elevation of  this ray in the sensor reference frame.    Now since the algorithm is working with a group of  at least two sensors, consider a pair of sensors {a,b} at  time � and a pair of measurements {za,�, zb,�}—one from  each sensor. Then consider the two corresponding line-  of-sight rays, one starting at the origin of sensor a ex-  tending in the direction indicated by za,� and the other  starting at the origin of sensor b extending in the direc-  tion indicated by zb,�. In the absence of clutter and mea-  surement error, and given that the two measurements  originated from the same target, this pair of rays would  provide the precise location of the target as discussed in  (for example) [1] and [8] by finding the point where the  rays intersect. With additive measurement errors, how-  ever, these rays are unlikely to intersect. Furthermore, if  one or both of the measurements are clutter originated,  or if they do not originate from the same target, then  the measurements do not have any meaning when con-  sidered as a pair. However, if the measurements in the  pair both happen to originate from a target at a point p  in the global space, then the rays indicated by the two  measurements should “closely agree” on a point near p,  though they will not have an exact intersect due to the  measurement errors.    Recall that at sample time �, a single sensor i returns  a “scan”—a set of measurements Zi,� that includes all  clutter-originated and target-originated measurements.  So for each possible pair of sensors, the hill finder rou-  tine should check for “hit points” in three-dimensional  space that both sensors “closely agree” upon.Obviously  not every pair of measurements will “closely agree” on  a point since many measurements are clutter-generated  and it is not guaranteed that the two sensors are even  “looking at” any of the same points (the sensors could,  for instance, be placed back-to-back and faced in oppo-  site directions). Thus, in a process of elimination, mea-  surement pairs are subjected to a series of increasingly    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 99        strict tests. First of all, the tests must eliminate from con-  sideration pairs of measurements that could not pos-  sibly correspond to a single target location in three-  dimensional space. Then, pairs that are likely to be un-  related (one or both are clutter originated or they orig-  inate from different targets) must be eliminated. And,  finally, any measurement pairs that remain must be con-  densed into composite point measurements in the three-  dimensional tracking space—a point p̂ upon which they  agree according to some criterion—and the “strength”  of agreement should be quantified so that the composite  measurements can be objectively ranked in quality.    1) Formation of Composite Point Measurements:  This series of tests is presented next for a single mea-  surement pair. The first level of tests are based on com-  puting the closest approach between the pair of line-of-  sight rays. This is heuristic but useful for computation-  ally cheap elimination of measurement pairs that are  most likely unrelated, which is especially useful in sce-  narios with large amounts of clutter in each scan. The fi-  nal,more strict test is based on the iterative least-squares  (ILS) estimator.    To formalize, consider a particular pair of sensors  {a,b}, a 	= b, at a particular time �. Take the pair of  scans {Za,�, Zb,�}, and let    Nzi,� = |Zi,�|, i = a,b, (27)  be the number of individual measurements in the scans.    Now consider some pair of measurements (one from  each scan)    {za,�,m, zb,�,n}, m ∈ {1, . . . ,Nza,�}, n ∈ {1, . . . ,Nzb,�}.  (28)    Let us momentarily drop the time (�) and measurement  pair (m,n) indexing and simply consider a particular  measurement from sensor a, call it za, a particular mea-  surement from sensor b, call it zb, with both measure-  ments taken at the same time.Eachmeasurement can be  taken to represent a ray (half line). So the pair of mea-  surements yields a pair of rays parameterized by a pair of  origin points and a pair of unit vectors that indicate the  rays’ pointing directions in the global coordinate system.    The origin of the first ray is ξa—the location of sensor  a, which is assumed to be known. Similarly, the second  ray has origin point ξb. The direction vectors are found  by first obtaining the unit vectors in their respective sen-  sor reference frames and transforming them both into  the global reference frame with the known sensor orien-  tations. In the case of azimuth–elevation measurements,  the unit direction vectors are given by    vi = �i  ⎡  ⎣ sinφisin θi cosφi  cos θi cosφi    ⎤  ⎦ i = a,b, (29)    where θi, φi are the individual components of zi and �i  is the rotation matrix of the ith sensor pose.    For any non-parallel pair of rays in three-  dimensional space, there is a single line segment    Fig. 5. Notional illustration of the points of closest approach (red  and blue points) between two line-of-sight measurements.    somewhere that connects the rays and is perpendic-  ular to both rays. The end points of this line segment are  the points of closest approach of the rays, and the length  is the minimum distance between the rays [19]. For this  case, solve for the end points of the minimum distance  segment with    pmdi = ρivi + ξi, i = a,b, (30)  where pmdi is the location of the closest approach that lies  on ray i. The scalar values ρa, ρb determine the distance  along each ray at which the closest approach occurs. Let    c � ξb − ξa, (31)  then these scalar values are given by    ρa =  −(va · vb)(vb · c) + (va · c)    1 − (va · vb)2  , (32)    ρb =  (va · vb)(va · c) − (vb · c)    1 − (va · vb)2  , (33)    where the dot indicates a vector dot product.Both values  are defined as long as the two measurement rays are not  perfectly parallel, which happens with probability zero.  Refer to Fig. 5 for an illustration of the closest approach  between an example pair of measurements.    First, a test can be performed by considering just the  signs of the scalar values ρa, ρb. These are the Carte-  sian distances along the rays where the closest approach  points occur. Thus, if either ρa or ρb is negative, it means  the corresponding closest approach point occurs “be-  hind the sensor.”(It is assumed that the negative z half of  the sensor coordinate frame is never observable.) So, if  either value is negative, reject the corresponding pair of  measurements as being indicative of a “hit” on a target.    Furthermore, in some settings, it would be reason-  able to set maximum limits for the values ρa, ρb such  that if either value exceeds its maximum, the pair of    100 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        0 1 2 3 4 5 6 7  0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1    H  1    H  0    2  1    Fig. 6. The empirical distributions of the normalized measurement  error squared under the binary hypotheses:H0: the measurements  used in the ILS estimate are unrelated andH1: the measurements  used in the ILS estimate originated from the same target. The    one-DOF Chi-squared distribution is also plotted for reference. The  discrepancy between the theoretical Chi-squared distribution and    empirical distribution under H1 is due to the thresholded termination  of the ILS. The distributions would match if the ILS was run to exact    termination with perfect numerical precision.    measurements is eliminated. For instance, if visibility is  such that the user knows that no sensor can see farther  than 10 000 m, and two measurement rays have a closest  approach point that is 15 000 m away from one of the  sensors, then that pair of measurements could also rea-  sonably be rejected as originating from a common target.  This would also be of use if it is known that the target  tracking space down range of one or both sensors has  a hard limit—e.g., the sensors are orbiting the earth at  some known altitude and are pointed toward the earth’s  surface.    -150 -100 -50 0 50 100 150  -150    -100    -50    0    50    100    150    Fig. 7. Example camera view of hits for a single scan for a clutter  level of λ = 30.    -150 -100 -50 0 50 100 150  -150    -100    -50    0    50    100    150    Fig. 8. Example camera view of superimposed scans for an entire  batch for a clutter level of λ = 30. Blue dots indicate true target hits  and red dots are clutter. Note that without the color coding, it is not  clearly evident to the human eye where the target track is located.    The blue outline shows the image edge that results from setting limits  on the azimuth and elevation angles.    A final heuristic test involves setting a threshold τmd  on the length of the minimum distance segment such  that if    ||pmda − pmdb ||2 ≥ τmd, (34)  then the measurement pair is eliminated from consider-  ation. The ideal value for τmd depends on the particular  scenario and the desired level of restraint in eliminating  measurement pairs. Qualitatively speaking, the farther  down range the targets are expected to appear, the  greater the τmd should be. And, if it is found that the  algorithm is considering more measurement pairs than  the user finds reasonable, then τmd can be decreased.  This is perhaps the most heuristic of the series of tests.  However, in simulated scenarios with large amounts of  clutter, and sensors, which are known to be observing a  common space in which the targets are known to exist, it  has been found to be the workhorse test that eliminates  all but the most likely-to-be-related measurement pairs.    The tests presented thus far serve to eliminate line-  of-sight measurement pairs that either do not point to-  ward a common space, point at a space that is physi-  cally too far away for the sensors to observe, point at a  space where targets are not likely to exist, or are likely  unrelated based on not approaching each other within  a reasonably constrained space. Now, any measurement  pairs that remain must be either eliminated as well or  consolidated into a single composite measurement point  in three-dimensional space. Suppose that the measure-  ment pair under consideration—{za, zb}—has passed the  simple tests involving the closest approach between the  lines-of-sight. If those tests were reasonably well-tuned  to the operating scenario, the fact that the measure-  ment pair has passed increases the likelihood that both    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 101        -100 -50 0 50 100    -100    -50    0    50    100    -100 -50 0 50 100    -100    -50    0    50    100    -100 -50 0 50 100    -100    -50    0    50    100    Fig. 9. The true hits received by each camera over the course of the entire scenario. Missed detections are accounted for (not visible).    measurements in the pair originated from a common tar-  get. Thus, a composite point measurement in the three-  dimensional tracking space based on the pair of mea-  surements is likely to be “meaningful”—an estimate of  a true target position.    The ILS estimator provides a mathematically rigor-  ous method to form a composite point measurement  based on the pair of line-of-sight measurements. It func-  tions by iteratively solving for the point p̂ in the three-  dimensional tracking space, which minimizes the nor-  malizedmeasurement error squared implied by themea-  surement pair due to a target at point p̂, which is a scalar  value given by    � =  ∑  i=a,b    (zi − z̄i(p̂))′R−1i (zi − z̄i(p̂)). (35)    where (·)′ denotes the matrix transpose. The term z̄i(p̂)  is the zero-error measurement returned by sensor i due  to a target at p̂. Notice the implicit assumption that both  of the measurements in the pair have in fact originated  from the same target—something that cannot be known  for certain in this case. However, due to the previous se-  ries of tests based on the closest approach points, the ILS  estimate will be calculated only for likely-to-be-related  measurement pairs. Furthermore, the final minimized  value of � will be used as a final test statistic to eliminate  all but the strongest composite measurements.    The ILS estimator using line-of-sight measurements  in azimuth–elevation form is given in [13], and used for  related work in [9]. It is summarized here. First, form a  4×1 vector by stacking the pair of measurement column  vectors    z =  [  za  zb    ]  (36)    and form the corresponding 4 × 4 covariance matrix    R =  [  Ra 0    0 Rb    ]  , (37)    where 0 is a 2 × 2 matrix of zeros. Then p̂q—the ILS es-  timate at the qth iteration—is updated with an additive    term as    p̂q+1 = p̂q + �q, (38)  where the additive update term is calculated as    �q =  [  Hq    ′  R−1Hq    ]−1  Hq    ′  R−1    [  z − z̄(p̂q)    ]  , (39)    where the 4 × 1 zero-error measurement vector is given  by stacking the individual zero-error measurement vec-  tors as    z̄(p̂q) =  [  z̄a(p̂q)  z̄b(p̂q)    ]  , (40)    in which the individual vectors z̄i(p̂q) are obtained by  first setting X j,� = p̂q in (14), and then substituting the  result into (9). The matrixHq, defined as    Hq =  ∂ z̄(X)    ∂X    ∣∣∣∣  X=p̂q    , (41)    is the 4 × 3 Jacobian matrix of the stacked zero-error  measurement vector with respect to Cartesian position  in the global reference frame, evaluated at the current  ILS estimate.    The formulas for the individual elements of the Ja-  cobian matrix and the initialization of the ILS estimator  are given in the Appendix.    The ILS estimator is terminated by setting a thresh-  old τ� and iterating until    ||�q||2 ≤ τ� (42)  and recording the final estimate as    p̂ = p̂q+1. (43)  Once the final ILS estimate is obtained, the mini-    mum normalized measurement error squared given by  (35) is theoretically Chi-squared distributed with one  DOF given that the pair of measurements used in the es-  timate originated from a common target [4]. If the pair of  measurements used in the estimate is unrelated despite  having passed the previous tests, the distribution of the  minimum normalized measurement error has no known  closed form. However, the distribution is obtained em-  pirically through simulation and plotted in Fig. 6. The    102 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        shape of this distribution is much like the Chi-squared  one DOF distribution, but with a much heavier tail. This  allows for a final thresholded test using �—calculated  with the formula in (35)—as the test statistic. That is,  if the value of � resulting from the ILS estimate based  on the measurement pair under consideration is above  some threshold τ� , then the pair and the resulting com-  posite measurement can be eliminated from considera-  tion.Otherwise, the composite measurement has passed  all elimination tests and is taken to correspond to an ac-  tual target. A composite measurement that passes this  final test can be assigned a quantitative score given by    σ = 1 − χ21 (�) (44)  —the complement of the one DOF Chi-squared cumu-  lative distribution function (cdf) evaluated at the mini-  mized normalized measurement error squared. This will  provide a score in the range [0, 1], with better estimates  receiving higher scores.    The elimination tests above were presented in terms  of a single pair ofmeasurements from a particular pair of  sensors (a,b) at a single time step, and the indexes of the  measurement pair (m,n) and time step (�) were omitted  throughout. Now, suppose that the algorithm performs  the entire series of elimination tests for every element  of the set    {{za,�,m, zb,�,n} : 1 ≤ m ≤ Nza,� 1 ≤ n ≤ Nzb,�}  —every possible pair of measurements from the scans of  sensors a and b at time �. Record to memory all the re-  sulting composite measurement points that have passed  all the elimination tests along with their corresponding  Chi-squared scores given by (44) as the set of parameter  pairs    P̃(a,b),� =  {  (p̂, σ )p    }Np  (a,b),�    p=1  , (45)    whereNp  (a,b),�    is the number of composite measurements  from sensor pair (a,b) at time step � that have not been  eliminated.    Notice that along with the obvious dependence on  time index �, P̃(a,b),� is also dependent on the pair (a,b).  That is, the list of points obtained at a particular time  depends on which pair of sensors is being used to look  for points. With a group of Nt > 2 sensors, find the list  of hit points from each possible pair of sensors and form  the total list    P� =  ⋃    (a,b)∈r  P̃(a,b),�, (46)    where r is the set of all unordered pairs of the integers  [1,Ns].    2) Formation of Initial Parameter Estimates: To form  initial parameter vectors to pass to the climber at esti-  mation time step k, suppose that P� has been obtained  for every time step � ≤ k. Potential target track seg-  ments over the current batch are formed by pairing com-    posite measurements from the current batch start time  in Pk−Nb+1 with composite measurements from the cur-  rent batch “leading edge” time in Pk, which parame-  terizes line segments in global three-dimensional space.  If the user has knowledge of a minimum and/or maxi-  mum possible target velocity (in units length/sampling  period), then it can be used here to select only the rea-  sonable potential track segments. The segments are also  assigned a score equal to the product of the Chi-squared  score of the composite measurements that make up its  end points. This is useful in quantitatively ranking which  segments are the best if the number of segments sent to  the hill climber must be limited.    Formally, let s = k − Nb + 1 be the batch start time  step, and form the set of parameter pairs given in (47),    Qk =  {(    Q =  [  p̂s,p  p̂k,q    ]  , δ = σs,pσk,q    )    : vminT  b ≤ ||p̂s,p − p̂k,q||2 ≤ vmaxTb,    1 ≤ p ≤ |P s|, 1 ≤ q ≤ |Pk|  }    , (47)    where p̂s,p is the pth composite measurement in the set  of composite measurements from the batch start time  step s, and σs,p is the measurement’s Chi-squared score.  Similarly, p̂k,q, σk,q are the individual members of the  qth element in the set of composite measurements from  the batch leading edge time step k. Let [vmin, vmax] rep-  resent the range of possible target velocities and Tb =  Nb − 1 be the duration of the batch in sampling periods.  Forgive the reuse of the index q—it was used in Section  IV.A.1 for an unrelated purpose.    To be verbose, each element of the setQk contains a  6 × 1 column vector parameterizing a line segment in  three-dimensional space that satisfies a length restric-  tion, paired with a scoring value that represents the “tar-  get indication strength” of the segment based on the  Chi-squared scores of the two composite point measure-  ments that parameterize the segment. Notice that only  composite measurements from the start and end time  steps of the batch are used to populate Qk. If the user  finds that this does not provide enough target track seg-  ment estimates, then pairs of composite measurements  from intermediate batch times can be used to form seg-  ments that are projected to the start and end batch times.  That is, select pairs of composite measurements, one  from time step t and one from time step �, such that  s ≤ t < � ≤ k and—based on the assumption of a  constant sampling period—compute the individual 3×1  components of the stacked vector Q in (47) as    p̂s = p̂� +  (    � − s  � − t    )  (p̂t − p̂�), (48)    p̂k = p̂t +  (  k− t  � − t    )  (p̂� − p̂t ), (49)    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 103        and assign the projected segment a score equal to the  product of the Chi-squared scores of the two compos-  ite measurements used in the calculation. This segment  projection is particularly useful in scenarios where all  sensors have a low probability of target detection in any  one scan since, for there to exist a meaningful composite  measurement at any one time step, at least two sensors  will need to have detected the same target at that sam-  pling instant.    The likelihood maximization will assume the num-  ber of targets based on the size of the initial parame-  ter vector it is passed. In order to choose a good ini-  tial vector that parameterizes Nt targets, sample Nt seg-  ments from Qk without replacement, and stack the in-  dividual segment vectors into a 6Nt × 1 parameter vec-  tor of the form given in (2). The sampling should give  preference to segments with high δ scores.A decent way  to accomplish this is to use a high-level function like  MATLAB’s datasample() and sample element r of Qk  with the weight of δr relative to all other δ values. If  the number of segments in Qk is small enough that it  will not be computationally prohibitive to simply run a  new instance of the hill climber for every possible ini-  tial parameter vector, then this sampling can be avoided  altogether.    Thus far, this section has discussed a method of using  observed data to initialize the hill climber. This has been  found to perform well enough as the sole method of  hill climber initialization at the beginning of each batch.  However, performance can be improved further if the  target motion for the current batch is predicted from the  best previous batch estimate (under the constant veloc-  ity assumption). The predicted segments along with seg-  ments sampled from Qk are then each used to initialize  individual instances of the hill climber. In simulation, it  was found that this prediction aids the tracker in “stick-  ing”to the track once it has a good estimate.On the other  hand, it cannot be used as the only method of initializing  the hill climber since the data-driven hill finder is needed  to converge on a good estimate in the first place and to  recover if the track is ever lost.    B. Hill Climbing    There are many types and variations of hill climbers.  The maximization in this work uses the conjugate gradi-  ent method implemented in Python’s SciPy “optimize”  library [24]. It should be pointed out that an elegant  expectation-maximization (EM) approach could also be  used (see [2]).    In some cases, the user might be interested in decid-  ing whether there is even a single target present or not  (target detection). In this case, one could pass the hill  climber an initial vector X k0 that implies a single target  (six elements), and allow the climber to run till it reaches  a peak. Then, compare the value of the LLR at this peak  to some threshold to decide if there is a target present.  That is, if the peak of hill is below some height, decide    that the hill is just due to a randomly occurring pattern  in clutter instead of an actual target. The challenge with  this is picking a good threshold for this test. The peak  height of a given LLR hill depends heavily on the num-  ber of sensors being used and the geometric arrange-  ment of the sensors relative to each other and the targets.  The simulated detection performance is discussed in  Section V.B.    C. Tracker Summary    Tracking is performed in a “sliding batch” fashion  where after an estimate is obtained, the leading edge of  the batch slides forward by some number of sampling  periods less than the length of the batch. This means  that consecutive batch estimates share some observa-  tions and are thus correlated. The following will give a  step-by-step synopsis of the algorithm:    1) At time step k, take the Nb most recent scans from  each of the Ns sensors to form the current measure-  ment batch. That is, k indicates the leading edge of  the batch.    2) For each time step in the batch, for each possible pair  of sensors, obtain via the process of elimination in  Section IV.A.1 pairs of measurements from the pair  of sensors that “strongly agree” on some point in the  global space. Record the resulting composite mea-  surements.    3) With composite measurements obtained in the pre-  vious step, form line segments representing target  tracks over the course of the batch. These segments  are parameterized by their start and end points.They  can be formed either by pairing hit points from the  start and end times of the batch or by considering  pairs of hit points from intermediate time steps and  projecting out to the end points of the batch. If infor-  mation is available aboutmaximum and/orminimum  target speed, this can be used to eliminate segments  that are either too long or too short.    4) If working with a multi-target scenario with Nt tar-  gets, then form potential target parameters by com-  bining line segments from the previous step into  groups of size Nt. In a single target scenario, any one  of the line segments can be taken as a potential target  parameter.    5) (Optional) Form a predicted target parameter by us-  ing a constant velocity assumption to predict the tar-  get trajectories over the current batch based on the  ML estimate from the previous batch.    6) For each of the parameter vectors obtained in 4) and  5), evaluate the LLR. Keep the N best parameters  according the LLR value.    7) Initialize N hill climbers with each of the N best pa-  rameters from the previous step. Allow climbers to  run until terminal condition or until some maximum  number of steps has been exceeded.Take the param-    104 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        eter indicated by the climber that reached the highest  peak in LLR to be the ML estimate for the current  batch.    8) Increment k by the desired batch slide amount. Re-  turn to 1).    It is of interest to summarize the computational  complexity of the method for a single batch estimate.  As presented, the expected computational complexity    of the hill finding is O  (  (λ + pdNt)2 N    s(Ns−1)  2    )  , where    λ + pdNt is the expected total number of measure-  ments per scan (clutter plus targets) and N    s(Ns−1)  2 is the    number of unique sensor pairs. That is, the point find-  ing (optimization-initialization, or hill finding) cost is  quadratic in both the number of measurements per scan  and in the number of sensors. While the expected num-  ber of measurements is usually dictated by “nature,” set-  tings withmany sensors may require care when choosing  which sensor pairs are used to find points.    The evaluation of the likelihood in (23) has expected  complexity O ((λ + pdNt)NsNt). The likelihood maxi-  mization requires approximation of the gradient of the  objective likelihood function via two-point differencing,  which requires 6Nt+1 evaluations of the objective func-  tion (one for each element of the parameter vector plus a  reference evaluation). Thus, the overall likelihood max-  imization has complexity O    ([  λ(Nt)2 + pd(Nt)3    ]  Ns    )  .    The number of targets assumed by the optimization is  the most significant factor in computational cost. While  (Nt)3 is the asymptotically dominant term, the entire ex-  pression λ(Nt)2 + pd(Nt)3 is noted since, in most practi-  cal settings, λ � pdNt (there is usually much more clut-  ter than target-originated measurements), so the clutter  level can dominate practical computation. Both the hill  finder andmaximizer also have simple linear complexity  in the batch length Nb.    V. RESULTS    The following sections present simulated results for  the sake of testing the presented method. The detection  performance (performance of a test to decide whether  or not a target is present) is given in Section V.B. The  tracking performance for a single target under various  states of nature is studied in Section V.C. In Section  V.D, a comparison is made to a decentralized method in  which each sensor obtains an ML estimate of the track  segment in its own measurement space, and then batch  estimates from pairs of sensors are fused to obtain track  segment estimates in the global space. This is in contrast  to the method presented in the main body of this work  could be considered a fuse-before-track method since  the likelihood involves measurements from all sensors.  Finally, a scenario with 2 targets is simulated, and a com-  parison is made between the tracking performance of  the presented method and the JPDA method presented  in [20].    A. Single Target Scenario Setup    The scenario has a single target and three sensors.  The target travels along an upward spiraling path at a  constant speed, see Fig. 9. Notice that, technically, the  curvature of the path violates the constant velocity as-  sumption in the target motion model. However, due to  the constant speed of the target and the small amount of  curvature that occurs over the course of any one batch,  constant velocity is a sufficient approximation.    Each simulated sensor has a restricted, conical field  of view with an angular range of 20◦. The sensors are  placed so that, for the majority of the target’s trajectory,  it is in view of all sensors.The sensor locations are all out  of view in Fig. 11, but referring to the coordinate system  in the figure:    � Sensor 1 is stationary at position (−18000,0,0), and  aimed in the +X direction (toward the origin) and  slightly up.Thismimics a camera viewing the sky from  the surface of the earth.    � Sensor 2 is stationary at position (0,0,60000), and  aimed toward the origin. This gives an overhead view  from a very high altitude.    � Sensor 3 is in motion. It orbits the Z axis at a height  of 12 000 above the X–Y plane and with a radius of  10 000. Its orientation changes so that it is consistently  aimed down and in toward the origin. Its orbital speed  is such that it makes only half an orbit throughout the  scenario.    In a real scenario, the expected number of clutter λ  and the probability of target detection pd are functions  of SNR and the threshold used in the measurement ex-  traction process. Qualitatively speaking, at a fixed SNR,  lower extraction threshold valueswill result inmore clut-  ter but higher probability of target detection.    For simulation purposes, it is assumed that the mea-  surement extraction process is Gaussian as in [11]. That  is, assume some extraction threshold τme, which, when  applied to a test for a measurement “hit” in some par-  ticular section of the sensor space where a target is not  present, results in a single clutter measurement with a  probability of “false alarm” given by    pf = Q  (  τme    σ    )  , (50)    where Q(·) is the Gaussian Q function and τme has been  normalized by themeasurement intensity standard devi-  ation σ . Then, for a fixed SNR, the probability of target  detection is    pd = Q(Q−1(pf ) − SNR). (51)  For the sake of simulation, it is assumed that there    are a Poisson random number of clutter measurements  in each scan with expected value λ, and the expected  number is approximately related to the clutter level by    pf =  λ    Nc  , (52)    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 105        0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1    SNR = 0 dB    SNR = -1 dB    Fig. 10. The ROC curves for target detection based on the  ML-PMHT likelihood at different SNR levels. The probability of  target detection at individual sensors is fixed at pd = 0.45. This    implies a clutter level of λ =∼54 at an SNR of 0 dB and λ =∼74 at an  SNR of −1 dB    where Nc represents some total number of resolved  “cells” in the sensor space that are tested by the mea-  surement extraction. It is assumed here that all sensors  have Nc = 400.    The simulation uses azimuth–elevation measure-  ments. Target-originated measurements have Gaussian  additive error where the individual components are as-  sumed independent and each has standard deviation  σθ = σφ = 0.25◦. The small measurement error vari-  ance is used to mimic the behavior of cameras, which  generally exhibit goodmeasurement accuracy.The other  “error-inducing” processes (clutter and missed detec-  tions) are considered more interesting in this context.    Unless otherwise noted, results use a fixed batch size  Nb = 18, and the batch slides five sampling periods from  one estimate to the next.    B. Detection Performance    It is of interest how well the algorithm does at de-  tecting the presence of a target.Detection would be per-  formed by choosing some threshold value and declaring  a detection if the LLR for the current batch exceeds the  threshold. To test the detection performance, the algo-  rithm is run for 100 Monte Carlo runs under the null  hypothesis—no target present.These data are combined  with the data from the simulationwith a target present to  form the receiver operating characteristic (ROC) curves  shown in Fig. 10.When the target is present, each sensor  measures it with fixed probability of detection pd = 0.45.    At first glance, the detection performance appears  poor. However, it is important to note that detection is  performed on a batch-to-batch basis.That is, if the detec-  tion was operating at a point on the ROC with PD = 0.3  and PFA = 0.03, then the user could expect to get a  detection within four batches of the target appearing,    0    500    2000    1000    1500Z    2000    -2000    2500    1000    3000    YX    00  -10002000    -2000    Fig. 11. The true target track (blue) plotted along with the batch  estimates (red) for a single Monte Carlo run with λ = 54 and    pd = 0.45.    while expecting a false detection only once in every ∼33  batches when there is no target present. When framed  in this manner, the detection performance is acceptable.  It is also important to emphasize that the detection and  false alarm probabilities on the axes of Fig. 10 are not  the same as the detection and false alarm probabilities  in (50) and (51), which are properties of the underlying  measurement extraction process.    C. Tracking Performance    When tracking with a single target present, the effect  of the hill finder can be seen in the first fewML batch es-  timates in Fig. 11.While the first few estimates are not on  track, the hill finder enables the tracker to converge on  the track within a handful of batches. If instead the hill  climbing was initialized randomly throughout the entire  tracking space, it would have a tendency to settle on false  hills instead of converging to the true track. There is a  point about one third of the way into the scenario where  the algorithm briefly does a poor job tracking the target.  This is due to the geometry of the target relative to the  sensors being less than ideal at that time—two sensors  have almost anti-parallel lines-of-sight on the target.The  resulting deviation in the estimate can be seen in Fig. 11  at the spot where the batch estimates drift away from  the true track and in Fig. 12 by the spike in estimation  error around time step 120. The spike in error is short-  lived, however, since the hill finder compensates as soon  as the sensors have good visibility on the target again.    We wish to also study the performance of the algo-  rithm in terms of how often it is on track. To do so, we  must first quantify what it means to be “on track.” One  way to define “on track” is to find the root mean square  error (RMSE) over the course of each batch. Then, if  the RMSE over the course of a particular batch is below  some threshold, declare the algorithm to be on track for  that batch. It is of interest to study the tracking perfor-  mance based on the operating characteristic of the mea-  surement extraction process. The SNR is fixed and sim-    106 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        0 50 100 150 200 250 300 350  0    500    1000    1500    2000    2500    3000    3500    Fig. 12. The RMS distance error between the estimate and true  target position versus time step. Averaged over 100 MC runs with    λ = 54 and pd = 0.45.    ulations are performed over a range of normalized mea-  surement extraction threshold values, which is equiva-  lent to a range of clutter levels via (50) and (52).The em-  pirical probability of the algorithm being on track (de-  notedPT) is plotted for three different SNRvalues in Fig.  13. The RMSE threshold used to declare whether the al-  gorithm is on track or not is set at 200 using the results  in Fig. 12—slightly higher than the RMSE to which the  algorithm empirically converges.    For each plot, a peak in tracking performance ap-  pears, above which the decreasing probability of target  detection has a negative effect on the hill finder, and be-  low which the increasing level of clutter results in de-  creasing estimation quality. The trend in the horizontal  location of the peaks suggests that the lower the SNR of    0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2  0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1    -2 dB  0 dB  2 dB    Fig. 13. The probability of being on track versus the normalized  measurement extraction threshold. Different plots are for various  measurement extraction SNR values. Results are averaged over 20  Monte Carlo runs of the scenario—or 1400 batches—for each data    point.    Fig. 14. Notional representation of fusing two batch estimates from  two separate cameras (red segments) into a single estimate in global    space (orange segment).    the tracking scenario, the lower the ideal measurement  extraction threshold. That is, in scenarios with lower  target visibility, it is better to operate in a “high clut-  ter/high detection probability” regime. And when tar-  gets are more visible, it is ideal to compromise on the  detection probability with the pay-off of having less clut-  ter. The vertical location of the peaks are perhaps less  informative since the probability of detection is defined  in terms of an RMSE threshold (a peak would be higher  if a higher error threshold was used for declaring the al-  gorithm on track).However, when the same threshold is  used for each data series in Fig. 13, the predictable trend  of increasing peak performancewith increasing SNRcan  be observed.    These results assume a Gaussian intensity detection  structure for the measurement extraction, which repre-  sents a “worst case.”Of course, if there is some other ex-  traction process with which a higher detection probabil-  ity and/or a lower level of clutter can be obtained (if, for  instance, the preprocessing of images involved software  that used features like shape or size to further discrimi-  nate between targets and clutter), then the performance  of the tracker will be better.    D. Track-Before-Fuse Comparison    The tracker presented in this paper maximizes the  likelihood of the target state directly in the global three-  dimensional space.An alternative approach is to make a  batch estimate of the track in the measurement space of  each individual sensor and then fuse them into a batch  estimate in the global three-dimensional space.For a no-  tional representation of this process, refer to Fig. 14. If  the sensors are cameras, one could obtain batch esti-  mates of the target track in the image space of each cam-  era (red segments in Fig. 14) using measurements from  only the camera under consideration. Then, given the  poses of a pair of the cameras, one could triangulate to  a corresponding batch estimate in the global space (or-  ange segments in Fig. 14). This approach may be con-    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 107        -1 -0.5 0 0.5 1 1.5 2 2.5 3    200    300    400    500    600    700    800    900    Global Space Tracker  Merged Pixel Space Tracker    Fig. 15. The RMS position error for the global space tracker (solid  line) and the RMSE for the image space tracker (dotted line) plotted  for implied measurement extraction SNR values. Errors are averaged    over 100 Monte Carlo runs and averaged over time.    sidered desirable in situations where communication of  data is restricted: Inmost realistic settings, the communi-  cation cost for sensors to send batches of measurement  scans to a central maximizer is higher than if each sen-  sor sends an ML estimate of a track segment. However,  this method has the obvious drawback of not having ac-  cess to the discriminatory power of the “hill finder” al-  gorithm in Section IV.A since the premise is that mea-  surement scans have not been sent to a central location.    We use this decentralized method as a benchmark  comparison at various SNR values. The expectation is  that the method presented in this paper should be more  robust—in terms of mean squared estimation error—as  the SNR decreases. This intuitive expectation is due to  the fact that both the hill finder and the centralized like-  lihood evaluation use the relative geometric arrange-  ment of the sensors, which creates a triangulation ef-  fect. In terms of the log likelihood, the effect is that the  hills climbed by the central tracker aremuch steeper and  taller than the hills in the individual sensor space track-  ers, which results in a better ML estimate from the cen-  tral tracker.    Monte Carlo simulations of the scenario described  in the previous section are performed as the SNR varies  from −1 to 3 dB. As the scenario progresses, both types  of tracking are performed simultaneously—tracking ac-  cording to the algorithm presented in this paper and  ML-PMHT tracking in the image spaces of the individ-  ual sensors and then fusing into three-dimensional space.  TheRMSEs obtainedwith the two differentmethods are  plotted versus the SNR in Fig. 15.    E. JPDA Comparison    A scenario with two targets is simulated in order to  test the multi-target capabilities of the ML method pre-  sented in this work. For the sake of comparison, the sce-    nario is also subjected to the JPDA tracker, which is a  typical recursive maximum a posteriorimethod that has  been adapted in [20] to work with multiple sensors.    The sensor arrangement remains the same as the  simulation in Section V.A. Along with the original tar-  get from the scenario in Section V.A, an additional  target is simulated that starts from rest at the point  (1000.0, 0.0, 0.0) in the global space and accelerates lin-  early to arrive at the point (−1000.0, 0.0, 2000.0) at the  end of the scenario. The linear path of the second target  remains well-resolved from the first target in the three-  dimensional global space, although the targets may be-  come unresolved in the two-dimensional measurement  space of any one sensor. Although both targets violate  the constant velocity assumption of the ML-PMHT, it  remains a close approximation over the batch duration,  which is shortened to Nb = 12 for this simulation. The  batch slide is also reduced to 1. That is, a batch estimate  is obtained at every time step based on the current scans  along with the scans from the past 11 time steps. This  ensures that the ML-PMHT is informed by every point  obtained by the hill finder routine.    Themeasurement error is the same as in Section V.A  (σθ = σφ = 0.25◦). The measurement extraction is as-  sumed to operate such that, for each target, the proba-  bility of receiving a hit at each sensor is pd = 0.5, and  the expected number of clutter measurements in each  scan of each sensor is λ = 50.    For the JPDA, a single linear white noise accelera-  tion model is assumed for the targets with the process  noise parameterization being informed by themaximum  acceleration exhibited by the true targets.    Both the JPDA and the ML-PMHT are initialized  with a random joint state estimate distributed (with large  variance) around the truth, although the ML-PMHT is  also allowed immediate access to the hill finder as well,  so it has a chance of finding a better initialization for its  first batch estimate.    Fig. 16 shows the root-mean-squared position esti-  mate error versus the time step of the scenario aver-  aged over both targets. The benefit of the point finder  employed by the ML-PMHT is evident at the begin-  ning of the scenario—while both trackers are given the  same random initialization, theML-PMHT immediately  makes use of the point finding, which, on average, re-  sults in faster convergence to the “steady-state” perfor-  mance. A test for statistical significance of the Monte  Carlo comparison, as given in [4], is as follows. For a par-  ticular Monte Carlo runmc, the RMSE from each of the  tracking methods is averaged over time, and the differ-  ence between the averages is noted as�mc.After all runs  are completed, the sample mean and sample standard  deviation of the “deltas” are computed, and the signifi-  cance of the comparison is taken to be the sample mean  divided by the sample standard deviation. A value � 2  is taken to indicate that the performance difference is  present in a significant number of runs. The comparison  in this simulation was found to have a significance of 4.7.    108 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        0 50 100 150 200 250 300 350  50    100    150    200    250    300    350    ML-PMHT    JPDA    Fig. 16. The RMS position error for the maximum-likelihood  tracker (solid line) and the RMSE for the JPDA (dotted line) plotted  versus time step. Errors are averaged over 100 Monte Carlo runs and    averaged over both targets.    Note that this scenario simulates a relatively low SNR,  and previous work has suggested that the ML-PMHT  likelihood formulation is more suitable than the JPDA  in such settings [14], [21].    VI. CONCLUSION    The algorithm presented in this work is found to  outperform a similar track-before-fusing algorithm at  reasonably low SNR levels. This result is intuitively pre-  dictable given that the global space tracker is taking ad-  vantage of knowing the geometric layout of the sensors  relative to each other when evaluating the observation  likelihood. It is also shown that there is a performance  “sweet spot” for the underlying measurement extraction  (the process by which “point hits” are declared) where  the target detection probability is high enough for the  likelihood maximization to be effective, but the level of  clutter does not overwhelm the batch estimation.    The “point finding” method presented in Section  IV.A, culminating in the list of weighted points in (46),  is not fundamentally tied to the ML-PMHT. Instead,  it represents a standalone method for obtaining a  list of points in three-dimensional Cartesian space by  fusing “likely-to-be-related” pairs of two-dimensional  line-of-sight measurements from different sensors. The  extracted points are used in this work to initialize the  likelihood maximization, and are found to help the  ML-PMHT stay on track.However, the point extraction  could also be used in other fundamentally different  settings. For instance, the “target birth” process and  two-point-differencing, which is essential to track ini-  tialization in the multi-Bernoulli formulation, could be  informed by the presented point finding method in a  tracking scenario where targets exist in R3 and multiple  sensors report two-dimensional measurements.    The formulation of the batch ML-PMHT presented  in this work allows for tracking multiple targets, and is  compared to the JPDA for a two-target scenario with  high clutter level and low probability of target detection.  TheML-PMHT is found to outperform the JPDA in this  case,which agrees with results in other works. In terms of  average computational cost, the ML-PMHT avoids the  expensive data-association step required by the JPDA,  but encounters the curse of dimensionality in the param-  eter space when dealing with multiple targets. Overall,  the results suggest that the ML-PMHT is a desirable op-  tion in settings where the user wishes to refine track es-  timates for relatively few targets in the presence of rel-  atively high levels of clutter and low detection probabil-  ity. Future work will insert the results of the ML-PMHT  to a generalized likelihood ratio test for target existence  using the threshold-setting techniques in [18], and will  compare to an automatically track-managed approach  such as the multi-Bernoulli filter [15].    APPENDIX A JACOBIAN COMPUTATION FOR ILS    Each iteration of the ILS estimator requires compu-  tation of (41): the Jacobian matrix of the stacked mea-  surement function of sensors a and b with respect to  Cartesian position of a target in the global space. In  keeping with the notation used in Sections II.B and  IV.A.1, this matrix is given by    Hq =    ⎡  ⎢⎢⎢⎢⎢⎢⎣    ∂θ̄a  ∂X    ∂θ̄a  ∂Y    ∂θ̄a  ∂Z    ∂φ̄a  ∂X    ∂φ̄a  ∂Y    ∂φ̄a  ∂Z    ∂θ̄b  ∂X    ∂θ̄b  ∂Y    ∂θ̄b  ∂Z    ∂φ̄b  ∂X    ∂φ̄b  ∂Y    ∂φ̄b  ∂Z    ⎤  ⎥⎥⎥⎥⎥⎥⎦    X=p̂q    , (53)    where X,Y,Z are the individual Cartesian coordinates  of the position vector X in the global reference frame.  Using the formula in (14), which transforms a point in  global coordinates into a point in the coordinate system  of sensor i, and, specifically, the rotation matrix defined  by (15), it can be shown that    Hq =  [  Hq,a �    ′  a    Hq,b �  ′  b    ]  , (54)    where �′i is the transpose of the rotationmatrix of sensor  i, and we have used the property of a rotationmatrix that  its inverse is equal to its transpose. The other sub-matrix  terms are given by    Hq,i =  ⎡  ⎣ ∂θ̄i∂xi ∂θ̄i∂yi ∂θ̄i∂zi    ∂φ̄i  ∂xi    ∂φ̄i  ∂yi    ∂φ̄i  ∂zi    ⎤  ⎦    xi=�′i(p̂q−ξi)    , (55)    which is the Jacobian of the measurement vector of  sensor i with respect to a point in the coordinate sys-  tem of sensor i, evaluated at the current ILS estimate  transformed to the coordinate system of sensor i. Tak-  ing the corresponding partial derivatives of (9) yields the    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 109        individual terms    ∂θ̄i    ∂xi  = 0, (56)    ∂θ̄i    ∂yi  = zi  y2i + z2i    , (57)    ∂θ̄i    ∂zi  = −yi  y2i + z2i    , (58)    ∂φ̄i    ∂xi  =    √  y2i + z2i    x2i + y2i + z2i  , (59)    ∂φ̄i    ∂yi  = −xiyi    (x2i + y2i + z2i )  √  y2i + z2i    , (60)    ∂φ̄i    ∂zi  = −xizi    (x2i + y2i + z2i )  √  y2i + z2i    , (61)    where xi, yi, zi are the individual Cartesian coordinates  in the sensor i reference frame. As indicated in (55), for  iteration q of the ILS, these terms are evaluated at the  coordinates of xi given by transforming the current ILS  estimate into the reference frame of sensor i with    xi = �′i(p̂q − ξi), (62)  which,when evaluated for both sensors a and b,will yield  Hq via (54).Note that the notation used here is as in Sec-  tion IV.A.1 where, since the ILS is being run using a sin-  gle pair of measurements taken at a single point in time,  the time indexing has been omitted. However, in gen-  eral, the sensor rotation matrices �i and positions ξi will  be time-dependent.    APPENDIX B ILS INITIALIZATION AND A  COMPUTATION-SAVING TRICK    The initial estimate p̂0 given to the ILS estimator can  be evaluated in various ways, and different works differ  in the initialization method. In this work, we found that  an initialization based on the closest approach between  the two line-of-sight measurements worked very well.  As discussed in Section IV.A.1, the closest approach will  have been previously evaluated for any measurement  pair being used in the ILS estimate.Thus, it costs no extra  computation to compute the initial estimate as    p̂0 =  ρa    ρa + ρb  (pmdb − pmda ) + pmda , (63)    where the positions pmdi for i = a,b are the end points of  the minimum distance segment connecting the lines-of-  sight indicated by measurements a and b evaluated via  (30). And, ρa, ρb are scalars given by (32) and (33), re-  spectively, which provide the Cartesian distance along  the lines-of-sight at which the minimum distance oc-  curs. Thus, (63) is a point on the minimum distance seg-  ment that lies proportionally closer to the line-of-sight    from the sensor that is physically closer to the closest  approach. The reasoning is that the farther the sensor  is from the target, the farther the corresponding line-  of-sight measurement will deviate from the target (in  Cartesian distance) due to some fixed measurement er-  ror. Thus,when considering a pair of sensors, the line-of-  sight from the sensor closer to the target tends to be—in  an expected value sense—more “trustworthy” in terms  of its Cartesian deviation from the target. This reason-  ing assumes that the measurement errors in each sensor  are identically distributed; if not, then the reasoning be-  comes less logical. The ILS estimate takes into account  non-identicalmeasurement error distributions,however,  and (63) still serves as a good initialization in such cases.    If the measurement errors from both sensors are  identically distributed, the position given by (63) has  been found to be a relatively good estimate of the tar-  get position, without even running ILS. Of course, ILS  will always provide further refinement of the estimate.  However, under certain conditions, the composite mea-  surements obtained in Section IV.A.1 via ILS can be re-  placed by simple evaluation of (63) for anymeasurement  pairs that pass the elimination tests. This would only be  done in the name of saving the computation power in-  volved in running ILS for each composite measurement,  and if one has computation power to spare, it is not a rec-  ommended compromise. However, in simulations with  a large number of sensors (Ns ≥ 3) and in which all  pairs of sensors have good cross-range confirmation (see  geometric dilution of precision in [4]), composite mea-  surements via (63) were found to serve the purposes of  this algorithm rather well. This is due to the fact that the  composite measurements, regardless of the level of re-  finement, are, in turn, used only for the initialization of  the batch estimate, which is further refined via the hill  climber. Thus, any lack of refinement in the composite  measurements from pairs of sensors is quickly made up  for once the batchML estimation is started using all sen-  sors. Qualitatively speaking, this is a good compromise  for scenarios with many sensors and small measurement  errors, becoming an increasingly poor practice with in-  creasing measurement error and/or decreasing number  of sensors.    If the user chooses to make the compromise laid  out above, the final thresholding test after ILS is run in  Section IV.A.1 can still be performed by replacing p̂ in  (35) with p̂0 from (63). However, no quantitative state-  ments can be made concerning the distribution of the  resulting normalized measurement error squared �0, re-  gardless of whether or not the measurement pair origi-  nated from a common target.That is, under the hypothe-  sis that the measurement pair did originate from a com-  mon target, it can no longer be claimed that the nor-  malized measurement error squared is Chi-squared dis-  tributed. However, referring to Fig. 17, it can be seen  that the shapes of the empirically obtained distributions  still lend themselves to a simple test by setting a thresh-  old such that, if the normalized estimation error squared    110 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        0 50 100 150 200 250  0    0.005    0.01    0.015    0.02    0.025    0.03    0.035    0.04    0.045    0.05    H  1    H  0    Fig. 17. The empirical distributions of the normalized measurement  error squared resulting from the unrefined composite measurement  under the binary hypotheses:H0: the line-of-sight measurements used  are unrelated and H1: the line-of-sight measurements used originated    from the same target.    exceeds the threshold, the composite measurement is re-  jected as being related to a target. However, compar-  ing to the distributions in Fig. 6, it can be seen that the  threshold used should be significantly larger than the  one used if the composite measurements are refined via  ILS. Furthermore, since the distribution under H1 is no  longer Chi-squared, the convenient score based on the  Chi-squared cdf in (44) is no longer valid for finalized  composite measurements. They must be ranked objec-  tively according to their normalized measurement error  squared directly, with smaller errors being considered  better.    REFERENCES    [1] S. Avidam and A. Shashua  “Trajectory triangulation: 3D reconstruction of moving  points from a monocular image,”  IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 4, pp.  348–357, Apr. 2000.    [2] D. Avitzour  “A maximum likelihood approach to data association,”  IEEE Trans Aerosp. Electron. Syst., vol. AES-28, no. 2, pp.  560–565, Apr. 1992.    [3] B. Balasingam, Y. Bar-Shalom, P.Willett, and K. Pattipati  “Maximum likelihood detection on images,” in Proc. 20th  Int. Conf. Inf. Fusion, Jul. 2017, pp. 1–8.    [4] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan  Estimation with Applications to Tracking and Navigation:  Theory, Algorithms and Software. Hoboken, NJ, USA:  Wiley, 2001.    [5] H. Coxeter  Projective Geometry. New York, NY,USA: Springer, 2003.    [6] A. Garcia-Fernandez, L. Svensson, J. Williams, Y. Xia, and  K. Granstrom  “Trajectory Poisson multi-Bernoulli filters,”  IEEE Trans. Signal Process., vol. 68, pp. 4933–4945, Aug.  2020.    [7] E. Giannopoulos, R. Streit, and P. Swaszek  “Probabilistic multi-hypothesis tracking in a multi-sensor,  multi-target environment,”    in Proc. 1st Australian Data Fusion Symp., Nov. 1996, pp.  184–189.    [8] R. Hartley and A. Zisserman  Multiple View Geometry in Computer Vision. Cambridge,  U.K.: Cambridge Univ. Press, 2003.    [9] M. Kowalski, P.Willett, T. Fair, and Y. Bar-Shalom  “CRLB for estimating time-varying rotational biases in  passive sensors,”  IEEE Trans. Aerosp. Electron. Syst., vol. 56, no. 1, pp.  343–355, Feb. 2020.    [10] M. Krieg and D. Gray  “Multi-sensor, probabilistic multi-hypothesis tracking,”  in Proc. 1st Australian Data Fusion Symp., Nov. 1996, pp.  153–158.    [11] Q. Lu, Y. Bar-Shalom, P.Willett, and B. Balasingam  “Measurement extraction for a point target from an optical  sensor,”  IEEE Trans Aerospace Electron. Syst., vol. 54, no. 6, pp.  2735–2745, Apr. 2018.    [12] K. G.Murty  “An algorithm for ranking all the assignments in order of  increasing cost,”  Operations Res., vol. 16, no. 3, pp. 682–687,May 1968.    [13] R. Osborne and Y. Bar-Shalom  “Statistical efficiency of composite position measurements  from passive sensors,”  IEEE Trans. Aerosp. Electron. Syst., vol. 49, no. 4, Oct. 2013,  pp. 2799–2806.    [14] C. Rago, P.Willett, and R. Streit  “A comparison of the JPDAF and PMHT tracking  algorithms,”  in Proc. Int. Conf. Acoust., Speech, Signal Process., May  1995, pp. 3571–3574.    [15] S. Reuter, B. T. Vo, B. N. Vo, and K. Dietmayer  “The labeled multi-Bernoulli filter,”  IEEE Trans. Signal Process., vol. 62, no. 12, pp. 3246–3260,  Jun. 2014.    [16] R. Streit and T. Luginbuhl  “A probabilistic multi-hypothesis tracking algorithm  without enumeration and pruning,” in Proc. 6th Joint  Service Data Fusion Symp., Jun. 1993.    [17] R. Streit and T. Luginbuhl  “Probabilistic multi-hypothesis tracking,” Naval Undersea  Warfare Center, Newport, RI, USA, Tech. Rep. 10,428, Feb.  1995.    [18] S. Schoenecker, P.Willett, and Y. Bar-Shalom  “Extreme-value analysis for ML-PMHT, Part 1: Threshold  determination for false track probability,”  IEEE Trans. Aerosp. Electron. Syst., vol. 50, no. 4, pp.  2500–2514, Oct. 2014.    [19] D. Sunday  Practical Geometry Algorithms With C++ Code.  Independently published, 2021.    [20] J. K. Tugnait  “Tracking of multiple maneuvering targets in clutter using  multiple sensors, IMM, and JPDA coupled filtering,”  IEEE Trans. Aerosp. Electron. Syst., vol. 40, no. 1, Jan. 2004,  pp. 320–330.    [21] P.Willett and S. Coraluppi  “MLPDA and ML-PMHT applied to some MSTWG data,”  in Proc. 9th Int. Conf. Inf. Fusion, Feb. 2007, pp. 1–8.    [22] Euler angles. (2016). Accessed: Jan. 10, 2019. [Online].  Available: https://en.wikipedia.org/wiki/Euler_angles).    [23] Pinhole camera model. (2018). Accessed: Feb. 10, 2019.  [Online]. Available: https://en.wikipedia.org/wiki/Pinhole_  camera_model.    [24] scipy.optimize.minimize. (2021). Accessed: Sep. 1, 2021.  [Online]. Available: https://docs.scipy.org/doc/scipy/  reference/generated/scipy.optimize.minimize.html    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 111    https://en.wikipedia.org/wiki/Euler_angles  https://en.wikipedia.org/wiki/Pinhole_camera_model  https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html      Zachariah Sutton received the B.Sc. degree in electrical engineering from the Uni-  versity of Connecticut, Storrs, CT, USA, in 2016. He is currently working toward  Ph.D. degree in the Electrical and Computer Engineering Department, University  of Connecticut. His research interests include signal processing, statistical modeling,  and target tracking.    Peter Willett (F’03) has been a faculty member in the Electrical and Computer En-  gineering Department,University of Connecticut, Storrs, CT,USA, since 1986. Since  1998, he has been a Professor.His primary areas of research have been statistical sig-  nal processing, detection,machine learning, communications, data fusion, and track-  ing. He was the Editor-in-Chief for the IEEE Signal Processing Letters from 2014  to 2016. He was the Editor-in-Chief for the IEEE Transactions on Aerospace and  Electronic Systems from 2006 to 2011 and AESS Vice President for Publications  from 2012 to 2014. From 1998 to 2005, he was an Associate Editor for three active  journals—IEEE Transactions on Aerospace and Electronic Systems (for data fusion  and target tracking) and IEEE Transactions on Systems, Man, and Cybernetics, and  parts A and B. He is remains an Associate Editor for the IEEE AES Magazine. He  is a member of the IEEE AESS Board of Governors and of the IEEE Signal Pro-  cessing Society’s Sensor-Array and Multichannel (SAM) technical committee (and  is now Chair).    Tim Fair, Toyon Senior Staff Analyst, Deputy Director Signal Processing, Principle  Investigator, received the B.Sc. and M.Sc. degrees in electrical and computer engi-  neering from the University of California, San Diego,CA,USA,with focus on signal  and image processing, in 2008 and 2009, respectively. He has worked with SAIC and  Johns Hopkins Applied Physics Lab, where he developed and analyzed detection,  tracking, association, and discrimination algorithms for next-generation ISR plat-  forms and Missile Defense Agency satellite systems and the Navy Aegis Ballistic  Missile Defense program. Since joining Toyon in 2012, he has focused on algorithm  development for image and video processing.As part of this work at Toyon,Mr. Fair  has developed solutions in the fields of target detection and tracking, trajectory es-  timation, target pose estimation, and more recently on methods for applying deep  learning and artificial intelligence in the military sector.    112 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021        Yaakov Bar-Shalom (F’84) received the B.Sc. and M.Sc. degrees in electrical engi-  neering from the Technion,Haifa, Israel, in 1963 and 1967, respectively, and the Ph.D.  degree in electrical engineering from Princeton University, Princeton, NJ, USA, in  1970.He is currently a Board of Trustees Distinguished Professor with the ECEDe-  partment andMarianne E.Klewin Professor with the University of Connecticut.His  current research interests are in estimation theory, target tracking, and data fusion.  He has published more than 650 papers and book chapters. He coauthored/edited  eight books, including Tracking and Data Fusion (YBS Publishing, 2011). He has  been elected Fellow of IEEE for “contributions to the theory of stochastic systems  and of multitarget tracking.” He served as an Associate Editor for the IEEE Trans-  actions on Automatic Control and Automatica. He was General Chairman of the  1985 ACC,General Chairman of FUSION 2000, President of ISIF in 2000 and 2002,  and Vice President for Publications from 2004 to 2013. Since 1995, he has been a  Distinguished Lecturer of the IEEEAESS.He is a corecipient of the M.Barry Carl-  ton Award for the best paper in the IEEE TAESystems in 1995 and 2000. In 2002,  he received the J. Mignona Data Fusion Award from the DoD JDL Data Fusion  Group. He is a member of the Connecticut Academy of Science and Engineering.  In 2008, he was awarded the IEEE Dennis J. Picard Medal for Radar Technologies  and Applications, and in 2012, the Connecticut Medal of Technology. He has been  listed by academic.research.microsoft (top authors in engineering) as #1 among the  researchers in aerospace engineering based on the citations of his work. He is the  recipient of the 2015 ISIF Award for a Lifetime of Excellence in Information Fu-  sion. This award has been renamed in 2016 as the Yaakov Bar-Shalom Award for a  Lifetime of Excellence in Information Fusion.He has the following Wikipedia page:  https://en.wikipedia.org/wiki/Yaakov Bar-Shalom.    ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 113    https://en.wikipedia.org/wiki/Yaakov   </p>
</div>
<div class="references">
<h2>References</h2>
<ul class="unstructured-references">
<li class="ref-1">S. Avidam and A. Shashua “Trajectory triangulation: 3D reconstruction of moving points from a monocular image,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 22, no. 4, pp. 348–357, Apr. 2000.</li>
<li class="ref-2">“A maximum likelihood approach to data association,” IEEE Trans Aerosp. Electron. Syst., vol. AES-28, no. 2, pp. 560–565, Apr. 1992.</li>
<li class="ref-3">“Maximum likelihood detection on images,” in Proc. 20th Int. Conf. Inf. Fusion, Jul. 2017, pp. 1–8.</li>
<li class="ref-4">Estimation with Applications to Tracking and Navigation: Theory, Algorithms and Software. Hoboken, NJ, USA: Wiley, 2001.</li>
<li class="ref-5">Projective Geometry. New York, NY,USA: Springer, 2003.</li>
<li class="ref-6">K. Granstrom “Trajectory Poisson multi-Bernoulli filters,” IEEE Trans. Signal Process., vol. 68, pp. 4933–4945, Aug. 2020.</li>
<li class="ref-7">“Probabilistic multi-hypothesis tracking in a multi-sensor, multi-target environment,” in Proc. 1st Australian Data Fusion Symp., Nov. 1996, pp. 184–189.</li>
<li class="ref-8">Multiple View Geometry in Computer Vision. Cambridge, U.K.: Cambridge Univ. Press, 2003.</li>
<li class="ref-9">“CRLB for estimating time-varying rotational biases in passive sensors,” IEEE Trans. Aerosp. Electron. Syst., vol. 56, no. 1, pp. 343–355, Feb. 2020.</li>
<li class="ref-10">“Multi-sensor, probabilistic multi-hypothesis tracking,” in Proc. 1st Australian Data Fusion Symp., Nov. 1996, pp. 153–158.</li>
<li class="ref-11">“Measurement extraction for a point target from an optical sensor,” IEEE Trans Aerospace Electron. Syst., vol. 54, no. 6, pp. 2735–2745, Apr. 2018.</li>
<li class="ref-12">“An algorithm for ranking all the assignments in order of increasing cost,” Operations Res., vol. 16, no. 3, pp. 682–687,May 1968.</li>
<li class="ref-13">“Statistical efficiency of composite position measurements from passive sensors,” IEEE Trans. Aerosp. Electron. Syst., vol. 49, no. 4, Oct. 2013, pp. 2799–2806.</li>
<li class="ref-14">“A comparison of the JPDAF and PMHT tracking algorithms,” in Proc. Int. Conf. Acoust., Speech, Signal Process., May 1995, pp. 3571–3574.</li>
<li class="ref-15">“The labeled multi-Bernoulli filter,” IEEE Trans. Signal Process., vol. 62, no. 12, pp. 3246–3260, Jun. 2014.</li>
<li class="ref-16">“A probabilistic multi-hypothesis tracking algorithm without enumeration and pruning,” in Proc. 6th Joint Service Data Fusion Symp., Jun. 1993.</li>
<li class="ref-17">“Probabilistic multi-hypothesis tracking,” Naval Undersea Warfare Center, Newport, RI, USA, Tech. Rep. 10,428, Feb. 1995.</li>
<li class="ref-18">“Extreme-value analysis for ML-PMHT, Part 1: Threshold determination for false track probability,” IEEE Trans. Aerosp. Electron. Syst., vol. 50, no. 4, pp. 2500–2514, Oct. 2014.</li>
<li class="ref-19">Practical Geometry Algorithms With C++ Code. Independently published, 2021.</li>
<li class="ref-20">“Tracking of multiple maneuvering targets in clutter using multiple sensors, IMM, and JPDA coupled filtering,” IEEE Trans. Aerosp. Electron. Syst., vol. 40, no. 1, Jan. 2004, pp. 320–330.</li>
<li class="ref-21">“MLPDA and ML-PMHT applied to some MSTWG data,” in Proc. 9th Int. Conf. Inf. Fusion, Feb. 2007, pp. 1–8.</li>
<li class="ref-22">Available: https://en.wikipedia.org/wiki/Euler_angles).</li>
<li class="ref-23"></li>
<li class="ref-24">camera_model.</li>
<li class="ref-25"></li>
<li class="ref-26">reference/generated/scipy.optimize.minimize.html ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 111 https://en.wikipedia.org/wiki/Euler_angles https://en.wikipedia.org/wiki/Pinhole_camera_model https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html Zachariah Sutton received the B.Sc. degree in electrical engineering from the Uni- versity of Connecticut, Storrs, CT, USA, in 2016. He is currently working toward Ph.D. degree in the Electrical and Computer Engineering Department, University of Connecticut. His research interests include signal processing, statistical modeling, and target tracking. Peter Willett (F’03) has been a faculty member in the Electrical and Computer En- gineering Department,University of Connecticut, Storrs, CT,USA, since 1986. Since 1998, he has been a Professor.His primary areas of research have been statistical sig- nal processing, detection,machine learning, communications, data fusion, and track- ing. He was the Editor-in-Chief for the IEEE Signal Processing Letters from 2014 to 2016. He was the Editor-in-Chief for the IEEE Transactions on Aerospace and Electronic Systems from 2006 to 2011 and AESS Vice President for Publications from 2012 to 2014. From 1998 to 2005, he was an Associate Editor for three active journals—IEEE Transactions on Aerospace and Electronic Systems (for data fusion and target tracking) and IEEE Transactions on Systems, Man, and Cybernetics, and parts A and B. He is remains an Associate Editor for the IEEE AES Magazine. He is a member of the IEEE AESS Board of Governors and of the IEEE Signal Pro- cessing Society’s Sensor-Array and Multichannel (SAM) technical committee (and is now Chair). Tim Fair, Toyon Senior Staff Analyst, Deputy Director Signal Processing, Principle Investigator, received the B.Sc. and M.Sc. degrees in electrical and computer engi- neering from the University of California, San Diego,CA,USA,with focus on signal and image processing, in 2008 and 2009, respectively. He has worked with SAIC and Johns Hopkins Applied Physics Lab, where he developed and analyzed detection, tracking, association, and discrimination algorithms for next-generation ISR plat- forms and Missile Defense Agency satellite systems and the Navy Aegis Ballistic Missile Defense program. Since joining Toyon in 2012, he has focused on algorithm development for image and video processing.As part of this work at Toyon,Mr. Fair has developed solutions in the fields of target detection and tracking, trajectory es- timation, target pose estimation, and more recently on methods for applying deep learning and artificial intelligence in the military sector. 112 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 16, NO. 2 DECEMBER 2021 Yaakov Bar-Shalom (F’84) received the B.Sc. and M.Sc. degrees in electrical engi- neering from the Technion,Haifa, Israel, in 1963 and 1967, respectively, and the Ph.D. degree in electrical engineering from Princeton University, Princeton, NJ, USA, in 1970.He is currently a Board of Trustees Distinguished Professor with the ECEDe- partment andMarianne E.Klewin Professor with the University of Connecticut.His current research interests are in estimation theory, target tracking, and data fusion. He has published more than 650 papers and book chapters. He coauthored/edited eight books, including Tracking and Data Fusion (YBS Publishing, 2011). He has been elected Fellow of IEEE for “contributions to the theory of stochastic systems and of multitarget tracking.” He served as an Associate Editor for the IEEE Trans- actions on Automatic Control and Automatica. He was General Chairman of the 1985 ACC,General Chairman of FUSION 2000, President of ISIF in 2000 and 2002, and Vice President for Publications from 2004 to 2013. Since 1995, he has been a Distinguished Lecturer of the IEEEAESS.He is a corecipient of the M.Barry Carl- ton Award for the best paper in the IEEE TAESystems in 1995 and 2000. In 2002, he received the J. Mignona Data Fusion Award from the DoD JDL Data Fusion Group. He is a member of the Connecticut Academy of Science and Engineering. In 2008, he was awarded the IEEE Dennis J. Picard Medal for Radar Technologies and Applications, and in 2012, the Connecticut Medal of Technology. He has been listed by academic.research.microsoft (top authors in engineering) as #1 among the researchers in aerospace engineering based on the citations of his work. He is the recipient of the 2015 ISIF Award for a Lifetime of Excellence in Information Fu- sion. This award has been renamed in 2016 as the Yaakov Bar-Shalom Award for a Lifetime of Excellence in Information Fusion.He has the following Wikipedia page: https://en.wikipedia.org/wiki/Yaakov Bar-Shalom. ML-PMH TRACKING IN THREE DIMENSIONS FROM MULTIPLE TWO-DIMENSIONAL SENSORS 113 https://en.wikipedia.org/wiki/Yaakov</li>
</ul>
</div>

</body>
</html>