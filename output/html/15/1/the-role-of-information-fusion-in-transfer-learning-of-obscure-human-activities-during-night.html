<html>
<body>
<div class="article-parents">
<ul class="article-journal-name">
<li>Journal of Advances in Information Fusion (JAIF)</li>
<li>Volume 15</li>
<li>Issue 1</li>
<li>Pages 49 - 56</li>
<li>June, 2020</li>
<li class="article-doi"><a href="https://doi.org/">https://doi.org/</a></li>
</ul>
</div>

<div class="article-header">
<h1 class="article-title">The Role of Information Fusion in Transfer Learning of Obscure Human Activities During Night</h1>
<p class="article-authors">Anwaar  Ulhaq</p>
</div>

<div class="article-url">
<a href="https://confcats_isif.s3.amazonaws.com/web-files/journals/entries/Pages%20from%20JAIF_v15_i1-5_The%20Role%20of%20Information%20Fusion.pdf">Read Article</a>
</div>
<div id="articleBody" class="article-section">
<h2 class="article-heading" id="Abstract">Abstract</h2>
<p>   1557-6418/20/$17.00 © 2020 JAIF    The Role of Information Fusion  in Transfer Learning of Obscure  Human Activities During Night    ANWAAR ULHAQ    Human actions are often tightly coupled with their context that    can play an important role in their modeling and understating. How-    ever, adverse lighting conditions and clutter can easily disrupt the vi-    sual context during night, especially in outdoor environments. This sit-    uation makes it difficult for any autonomous system to detect or clas-    sify actions. Various works have proposed contextual enhancement of    available imagery to improve performance. However, no study articu-    lates the most suitable type of contextual enhancement. In this study,    we try to evaluate the role of information fusion in enhancing the vi-    sual context.We are interested in knowing whether fusion can enhance    the performance of the autonomous system or it is just visually appeal-    ing.Our evaluation framework is based on transfer learning using deep    convolutional neural networks.Experimental results show that contex-    tual enhancement based on 1) the fused contextual information and 2)    its colorization significantly enhances the performance of automated    action recognition.   </p>
</div>
<div class="references">
<h2>References</h2>
<ul class="unstructured-references">
<li class="ref-1">J. Liu, J. Luo, and M. Shah “Recognizing realistic actions from videos ‘in the wild’,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 1996–2003.</li>
<li class="ref-2">“Recognition of group activities using dynamic probabilistic networks,” in Proc. IEEE Int. Conf. Comput. Vis., 2003, pp. 742–749.</li>
<li class="ref-3">“Action detection in crowd,” in Proc. Brit. Mach. Vis. Conf., 2010, pp. 1–11.</li>
<li class="ref-4">“Learning realistic human actions from movies,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2008, pp. 1–8.</li>
<li class="ref-5">M. Zia “Single- and two-person action recognition based on silhouette shape and optical point descriptors,” Signal Image Video Process., vol. 12, no. 5, pp. 853–860, 2018.</li>
<li class="ref-6">“Learning spatio-temporal features for action recognition from the side of the video,” Signal Image Video Process., vol. 10, no. 1, pp. 199–206, 2016.</li>
<li class="ref-7">“On space–time filtering framework for matching human actions across different viewpoints,” 54 JOURNAL OF ADVANCES IN INFORMATION FUSION VOL. 15, NO. 1 JUNE 2020 IEEE Trans. Image Process., vol. 27, no. 3, pp. 1230–1242, 2018.</li>
<li class="ref-8">“On dynamic scene geometry for view-invariant action matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 3305–3312.</li>
<li class="ref-9">“On temporal order invariance for view-invariant action recognition,” IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 2, pp. 203–211, 2012.</li>
<li class="ref-10">“Making action recognition robust to occlusions and viewpoint changes,” in Computer Vision–ECCV. Berlin, Germany: Springer, 2010, pp. 635–648.</li>
<li class="ref-11">“What, where and who? Classifying events by scene and object recognition,” in Proc. IEEE 11th Int. Conf. Comput. Vis., 2007, pp. 1–8.</li>
<li class="ref-12">“Actions in context,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 2929–2936.</li>
<li class="ref-13">“Selection and context for action recognition,” in Proc. Int. Conf. Comput. Vis., 2009, vol. 9, pp. 1933–1940.</li>
<li class="ref-14">“Modeling scene and object contexts for human action retrieval with few examples,” IEEE Trans. Circuits Syst. Video Technol., vol. 21, no. 5, pp. 674–681, 2011.</li>
<li class="ref-15">“Hierarchical attention and context modeling for group activity recognition,” in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2018, pp. 1328–1332.</li>
<li class="ref-16">“Hierarchical attention network for action segmentation,” Pattern Recognit. Lett., vol. 131, pp. 442–448, 2020.</li>
<li class="ref-17">“Human activity recognition in thermal infrared imagery,” in Proc. IEEE Comput. Vis. Pattern Recognit. Workshop, 2005, pp. 17–17.</li>
<li class="ref-18">“Application of thermal infrared imagery in human action recognition,” Adv.Mater. Res., vol. 121, pp. 368–372, 2010.</li>
<li class="ref-19">“Contextual action recognition in multi-sensor nighttime video sequences,” in Proc. Digit. Image Comput. Techn. Appl., 2011, pp. 256–261.</li>
<li class="ref-20">“Action recognition in the dark via deep representation learning,” in Proc. IEEE Int. Conf. Image Process., Appl. Syst., 2018, pp. 131–136.</li>
<li class="ref-21">“Action recognition using spatio-temporal distance classifier correlation filter,” in Proc. Int. Conf. Digit. Image Comput. Techn. Appl., 2011, pp. 474–479.</li>
<li class="ref-22">“FACE: fully automated context enhancement for night-time video sequences,” J. Vis. Commun. Image Representation, vol. 40, pp. 682–693, 2016.</li>
<li class="ref-23">“Scarf: semi-automatic colorization and reliable image fusion,” in Proc. Int. Conf. Digit. Image Comput. Techn. Appl., 2010, pp. 435–440.</li>
<li class="ref-24">“An optimized image fusion algorithm for night-time surveillance and navigation,” in Proc. IEEE Symp. Emerg. Technol., 2005, pp. 138–143.</li>
<li class="ref-25">“Perceptual evaluation of colorized nighttime imagery,” in Proc. IS&T/SPIE Electron. Imaging, 2014, pp. 901412– 901412.</li>
<li class="ref-26">“Contextual action recognition with R∗CNN,” in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1080–1088.</li>
<li class="ref-27">“Imagenet classification with deep convolutional neural networks,” in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 1097–1105.</li>
<li class="ref-28">“3D convolutional neural networks for human action recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1, pp. 221–231, 2013.</li>
<li class="ref-29">“Application of thermal infrared imagery in human action recognition,” Adv.Mater. Res., vol. 121, pp. 368–372, 2010.</li>
<li class="ref-30">“Human activity recognition in thermal infrared imagery,” in Proc. IEEE Comput. Vis. Pattern Recognit. Workshop, 2005, pp. 17–17.</li>
<li class="ref-31">“RGB-IR cross input and sub-pixel upsampling network for infrared image superresolution,” Sensors, vol. 20, no. 1, p. 281, 2020.</li>
<li class="ref-32">“Deep visible and thermal image fusion for enhanced pedestrian visibility,” Sensors, vol. 19, no. 17, p. 3727, 2019.</li>
<li class="ref-33">“Automated multi-sensor color video fusion for nighttime video surveillance,” in Proc. IEEE Symp. Comput. Commun., 2010, pp. 529–534.</li>
<li class="ref-34">“Context enhancement to reveal a camouflaged target and to assist target localization by fusion of multispectral surveillance videos,” Signal Image Video Process., vol. 7, no. 3, pp. 537–552, 2013.</li>
<li class="ref-35">“How transferable are features in deep neural networks?” in Proc. Adv. Neural Inf. Process. Syst., 2014, pp. 3320–3328.</li>
<li class="ref-36">“UCF101: a dataset of 101 human action classes from videos in the wild.,” CRCV-TR-12-01, November, 2012.</li>
<li class="ref-37">“Multivariant technique for multiclass pattern recognition,” Appl. Opt., vol. 19, no. 11, pp. 1758–1761, 1980.</li>
<li class="ref-38">“Action-02MCF: a robust space–time correlation filter for action recognition in clutter and adverse lighting conditions,” in Proc. Int. Conf. Adv. Concepts Intell. Vis. Syst. Berlin, Germany: Springer, 2016, pp. 465–476.</li>
<li class="ref-39">A. Cardinali, C. Canagarajah, D. Bull, T. Riley, D. Hickman, THE ROLE OF INFORMATION FUSION IN TRANSFER LEARNING OF OBSCURE HUMAN ACTIVITIES DURING NIGHT 55 and M. I. Smith “The Eden Project multisensory data set,” The Online Resource for Research in Image Fusion (ImageFusion.org), 2006.</li>
<li class="ref-40">A.G. Hauptmann “InfAR dataset: infrared action recognition at different times,” Neurocomputing, vol. 212, pp. 36–47, 2016.</li>
<li class="ref-41">Correlation Pattern Recognition. New York, NY, USA: Cambridge University Press, 2005.</li>
</ul>
</div>

</body>
</html>